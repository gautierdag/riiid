activation: relu
batch_size: 256
dim_feedforward: 256
dropout: 0.0
emb_dim: 64
learning_rate: 0.001
max_window_size: 100
n_content_id: 13524
n_decoder_layers: 2
n_encoder_layers: 2
n_heads: 1
n_part: 8
n_tags: 189
num_user_train: 300000
num_user_val: 30000
validate_on_sequence: true
