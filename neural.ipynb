{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.style as style\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "style.use(\"fivethirtyeight\")\n",
    "import gc\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "SEED = 69\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lectures arrays\n",
      "Loading questions arrays\n",
      "CPU times: user 27.7 ms, sys: 965 ms, total: 992 ms\n",
      "Wall time: 991 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This code is needed for dataset\n",
    "train = pd.read_pickle(\"riiid_train.pkl.gzip\")\n",
    "questions_df = pd.read_csv(\"data/questions.csv\")\n",
    "lectures_df = pd.read_csv(\"data/lectures.csv\")\n",
    "\n",
    "folder_path = \"data\"\n",
    "print(\"Loading lectures arrays\")\n",
    "lectures_ids = np.load(f\"{folder_path}/lectures_ids.npy\")\n",
    "lectures_parts = np.load(f\"{folder_path}/lectures_parts.npy\")\n",
    "lectures_types = np.load(f\"{folder_path}/lectures_types.npy\")\n",
    "lectures_tags = lectures_df.tag.values\n",
    "\n",
    "print(\"Loading questions arrays\")\n",
    "questions_parts = np.load(f\"{folder_path}/questions_parts.npy\")\n",
    "questions_lectures_parts = np.concatenate([questions_parts, lectures_parts])\n",
    "\n",
    "\n",
    "# process tags\n",
    "def split_tags(t):\n",
    "    try:\n",
    "        return [int(i) for i in t.split(\" \")]\n",
    "    except AttributeError:\n",
    "        return list()\n",
    "\n",
    "\n",
    "# Get tags to be 2D array of shape (Q, T), where Q is question_idx, and T is the max number of tag possible (6)\n",
    "questions_df[\"tags\"] = questions_df.tags.apply(split_tags)\n",
    "questions_tags = pd.DataFrame(questions_df[\"tags\"].tolist(), index=questions_df.index)\n",
    "\n",
    "# map lecture id to new id\n",
    "\n",
    "lectures_mapping = dict(\n",
    "    zip(lectures_df.lecture_id.values, (lectures_df.index + 13523).values)\n",
    ")\n",
    "lectures_df.lecture_id = lectures_df.index + 13523\n",
    "lectures_tags = pd.DataFrame(\n",
    "    lectures_df.tag.values, index=lectures_df.lecture_id.values\n",
    ")\n",
    "\n",
    "questions_lectures_tags = pd.concat([questions_tags, lectures_tags])\n",
    "# pad with max tag + 1\n",
    "questions_lectures_tags = (\n",
    "    questions_lectures_tags.fillna(questions_lectures_tags.max().max() + 1)\n",
    "    .astype(np.int)\n",
    "    .values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    \"\"\"\n",
    "    Converts the lecture ids to proper content_ids\n",
    "    Adds the answered_correctly column if not exists\n",
    "    \"\"\"\n",
    "    df.content_type_id = df.content_type_id.astype(bool)\n",
    "\n",
    "    # prior information\n",
    "    df.prior_question_elapsed_time = (\n",
    "        df.prior_question_elapsed_time.fillna(0).clip(upper=300000) / 300000\n",
    "    )  # normalizes to 0-1\n",
    "\n",
    "    # map lecture ids to new content_ids\n",
    "    df.loc[df.content_type_id, \"content_id\"] = df[df.content_type_id].content_id.map(\n",
    "        lectures_mapping\n",
    "    )\n",
    "    # if not answered correctly then add column with\n",
    "    # y = 3 (padding) for all questions and y = 4 for lectures\n",
    "    if \"answered_correctly\" not in df.columns:\n",
    "        df[\"answered_correctly\"] = df.content_type_id.map({False: 3, True: 4})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0000001\n",
    "def get_time_elapsed_from_timestamp(arr):\n",
    "    arr_seconds = np.diff(arr, prepend=0) / 1000\n",
    "    return (np.log(arr_seconds + eps).astype(np.float32)-3.5) / 20\n",
    "\n",
    "# ## Show effects of transformation \n",
    "# times = train.groupby(\"user_id\").timestamp.apply(get_time_elapsed_from_timestamp)\n",
    "# times = np.concatenate(times.tolist())\n",
    "# sns.boxplot(x=times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Hdf5\n",
    "\n",
    "Only run this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.01 s, sys: 302 ms, total: 1.32 s\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = preprocess_df(train) #convert lecture ids to be sequentially following the question_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['row_id', 'timestamp', 'user_id', 'content_id', 'content_type_id',\n",
       "       'task_container_id', 'user_answer', 'answered_correctly',\n",
       "       'prior_question_elapsed_time', 'prior_question_had_explanation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.answered_correctly.replace(-1, 4, inplace=True) # set lecture to token 4 for answered correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41257e0404542f990c1242fc1f0e279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=338170.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ignore lectures for now\n",
    "hf = h5py.File(\"feats.h5\", \"w\")\n",
    "\n",
    "for user_id, data in tqdm(train.groupby(\"user_id\")):\n",
    "    processed_feats = data[\n",
    "        [\n",
    "            \"content_id\",\n",
    "            \"answered_correctly\",\n",
    "            \"timestamp\",\n",
    "            \"prior_question_elapsed_time\",\n",
    "        ]\n",
    "    ].values\n",
    "\n",
    "    hf.create_dataset(f\"{user_id}/content_ids\", data=processed_feats[:, 0], maxshape=(None,))\n",
    "    hf.create_dataset(f\"{user_id}/answered_correctly\", data=processed_feats[:, 1], maxshape=(None,))\n",
    "    hf.create_dataset(f\"{user_id}/timestamps\", data=processed_feats[:, 2], maxshape=(None,))\n",
    "    hf.create_dataset(f\"{user_id}/prior_question_elapsed_time\", data=processed_feats[:, 3], maxshape=(None,))\n",
    "\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Here we define the pytorch Dataset object and a custom collate function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RIIDDataset(Dataset):\n",
    "    \"\"\"RIID dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_mapping,\n",
    "        user_history,\n",
    "        hdf5_file=\"feats.h5\",\n",
    "        window_size=100,\n",
    "        use_cache=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_mapping (np.array): array of user_ids per row\n",
    "            user_history (np.array): array of length of history up till this row\n",
    "            hdf5_file (string): location of hf5 feats file\n",
    "            window_size (int): size of window to lookback to max\n",
    "            use_cache (opt, bool): whether to cache reads\n",
    "        \"\"\"\n",
    "        # np array where index maps to a user id\n",
    "        self.user_mapping = user_mapping\n",
    "        self.user_history = user_history\n",
    "        self.hdf5_file = f\"{hdf5_file}\"\n",
    "        self.max_window_size = window_size\n",
    "        self.use_cache = use_cache\n",
    "        self.cache = {}\n",
    "\n",
    "    def open_hdf5(self):\n",
    "        # opens the h5py file\n",
    "        self.f = h5py.File(self.hdf5_file, \"r\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_mapping)\n",
    "\n",
    "    def load_user_into_cache(self, user_id):\n",
    "        \"\"\"\n",
    "        add a user to self.cache\n",
    "        \"\"\"\n",
    "\n",
    "        self.cache[user_id] = {\n",
    "            \"content_ids\": np.array(self.f[f\"{user_id}/content_ids\"], dtype=np.int64),\n",
    "            \"answered_correctly\": np.array(\n",
    "                self.f[f\"{user_id}/answered_correctly\"], dtype=np.int64\n",
    "            ),\n",
    "            \"timestamps\": np.array(self.f[f\"{user_id}/timestamps\"], dtype=np.float32),\n",
    "            \"prior_question_elapsed_time\": np.array(\n",
    "                self.f[f\"{user_id}/prior_question_elapsed_time\"], dtype=np.float32\n",
    "            ),\n",
    "        }\n",
    "    def preload_all(self):\n",
    "        if not hasattr(self, \"f\"):\n",
    "            self.open_hdf5()\n",
    "        for u in tqdm(np.unique(dataset.user_mapping)):\n",
    "            self.load_user_into_cache(u)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # open the hdf5 file in the iterator to allow multiple workers\n",
    "        # https://github.com/pytorch/pytorch/issues/11929\n",
    "        if not hasattr(self, \"f\"):\n",
    "            self.open_hdf5()\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        user_id = self.user_mapping[idx]\n",
    "        length = self.user_history[idx]\n",
    "        # length = self.f[f\"{user_id}/answered_correctly\"].len()\n",
    "\n",
    "        window_size = min(self.max_window_size, length)\n",
    "\n",
    "        # index for loading larger than window size\n",
    "        start_index = 0\n",
    "        if length > window_size:\n",
    "            # randomly select window size subset instead of trying to cram in everything\n",
    "            start_index = length - window_size\n",
    "\n",
    "        if not self.use_cache:\n",
    "            content_ids = np.zeros(window_size, dtype=np.int64).copy()\n",
    "            answered_correctly = np.zeros(window_size, dtype=np.int64).copy()\n",
    "            timestamps = np.zeros(window_size, dtype=np.float32).copy()\n",
    "            prior_q_times = np.zeros(window_size, dtype=np.float32).copy()\n",
    "\n",
    "            self.f[f\"{user_id}/content_ids\"].read_direct(\n",
    "                content_ids,\n",
    "                source_sel=np.s_[start_index : start_index + window_size],\n",
    "                dest_sel=np.s_[0:window_size],\n",
    "            )\n",
    "            self.f[f\"{user_id}/answered_correctly\"].read_direct(\n",
    "                answered_correctly,\n",
    "                source_sel=np.s_[start_index : start_index + window_size],\n",
    "                dest_sel=np.s_[0:window_size],\n",
    "            )\n",
    "            self.f[f\"{user_id}/timestamps\"].read_direct(\n",
    "                timestamps,\n",
    "                source_sel=np.s_[start_index : start_index + window_size],\n",
    "                dest_sel=np.s_[0:window_size],\n",
    "            )\n",
    "            self.f[f\"{user_id}/prior_question_elapsed_time\"].read_direct(\n",
    "                prior_q_times,\n",
    "                source_sel=np.s_[start_index : start_index + window_size],\n",
    "                dest_sel=np.s_[0:window_size],\n",
    "            )\n",
    "        else:\n",
    "            if user_id not in self.cache:\n",
    "                self.load_user_into_cache(user_id)\n",
    "\n",
    "            content_ids = self.cache[user_id][\"content_ids\"][\n",
    "                start_index : start_index + window_size\n",
    "            ].copy()\n",
    "            answered_correctly = self.cache[user_id][\"answered_correctly\"][\n",
    "                start_index : start_index + window_size\n",
    "            ].copy()\n",
    "            timestamps = self.cache[user_id][\"timestamps\"][\n",
    "                start_index : start_index + window_size\n",
    "            ].copy()\n",
    "            prior_q_times = self.cache[user_id][\"prior_question_elapsed_time\"][\n",
    "                start_index : start_index + window_size\n",
    "            ].copy()\n",
    "\n",
    "        # convert timestamps to time elapsed\n",
    "        time_elapsed_timestamps = get_time_elapsed_from_timestamp(timestamps)\n",
    "\n",
    "        # get question tags\n",
    "        tags = questions_lectures_tags[content_ids, :]\n",
    "\n",
    "        # get question parts\n",
    "        parts = questions_lectures_parts[content_ids]\n",
    "\n",
    "        # shift by one the answered_correctly sequence\n",
    "        answers = np.roll(answered_correctly, 1)\n",
    "\n",
    "        # set start token if start_index is actually first element\n",
    "        if start_index == 0:\n",
    "            answers[0] = 2\n",
    "        # else replace first element of sequence with actual previous element\n",
    "        else:\n",
    "            self.f[f\"{user_id}/answered_correctly\"].read_direct(\n",
    "                answers, source_sel=np.s_[start_index - 1], dest_sel=np.s_[0],\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"parts\": torch.from_numpy(parts).long(),\n",
    "            \"tags\": torch.from_numpy(tags).long(),\n",
    "            \"content_ids\": torch.from_numpy(content_ids),\n",
    "            \"answered_correctly\": torch.from_numpy(answered_correctly),\n",
    "            \"answers\": torch.from_numpy(answers),\n",
    "            \"timestamps\": torch.from_numpy(time_elapsed_timestamps),\n",
    "            \"prior_q_times\": torch.from_numpy(prior_q_times),\n",
    "            \"length\": window_size,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# The collate function is used to merge individual data samples into a batch\n",
    "# It handles the padding aspect\n",
    "def collate_fn(batch):\n",
    "    # collate lenghts into 1D tensor\n",
    "    items = {\"length\": torch.tensor([batch_item[\"length\"] for batch_item in batch])}\n",
    "\n",
    "    # find shape that the batch will have\n",
    "    max_length = items[\"length\"].max()\n",
    "    num_items = len(batch)\n",
    "\n",
    "    # padding list\n",
    "    for (key, padding) in [\n",
    "        (\"parts\", 0),\n",
    "        (\"content_ids\", 13942),\n",
    "        (\"answered_correctly\", 3),\n",
    "        (\"answers\", 3),\n",
    "        (\"timestamps\", 0.0),  # note timestamps isnt an embedding\n",
    "        (\"tags\", 188),\n",
    "        (\"prior_q_times\", 0),\n",
    "    ]:\n",
    "        items[key] = pad_sequence(\n",
    "            [batch_item[key] for batch_item in batch],\n",
    "            batch_first=False,\n",
    "            padding_value=padding,\n",
    "        )\n",
    "\n",
    "    # mask to weight loss by (S, N)\n",
    "    items[\"loss_mask\"] = (\n",
    "        (\n",
    "            torch.arange(max_length).expand(num_items, max_length)\n",
    "            < items[\"length\"].unsqueeze(1)\n",
    "        )\n",
    "        .transpose(1, 0)\n",
    "        .float()\n",
    "    )\n",
    "    items[\"loss_mask\"] *= items[\"answered_correctly\"] != 4  # mask the lectures\n",
    "    items[\"answered_correctly\"] = items[\"answered_correctly\"].float()\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "max_window_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset + Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.89 s, sys: 953 ms, total: 3.84 s\n",
      "Wall time: 3.85 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101230332"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Create Dataset with all rows\n",
    "user_mapping = train.user_id.values\n",
    "user_history = train.groupby(\"user_id\").cumcount().values + 1\n",
    "\n",
    "dataset = RIIDDataset(\n",
    "    user_mapping, user_history, hdf5_file=\"feats.h5\", window_size=100, use_cache=True\n",
    ")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.6 s, sys: 3.52 s, total: 21.2 s\n",
      "Wall time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## if not using lectures\n",
    "train = train.reset_index(drop=True)\n",
    "q_train_indices = train[\n",
    "    train.row_id.isin(cv_train_ids[~cv_train_ids.content_type_id].row_id.values)\n",
    "].index.values\n",
    "\n",
    "q_valid_indices = train[\n",
    "    train.row_id.isin(cv_valid_ids[~cv_valid_ids.content_type_id].row_id.values)\n",
    "].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if using lectures\n",
    "# question_ids = train[~train.content_type_id].row_id.values\n",
    "# q_train_indices = np.intersect1d(question_ids, cv_train_ids)\n",
    "# q_valid_indices = np.intersect1d(question_ids, cv_valid_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# Init DataLoader from RIIID Dataset subset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=Subset(dataset, q_train_indices),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=6,\n",
    "    pin_memory=torch.cuda.is_available(),  # if GPU then pin memory for perf\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=Subset(dataset, q_valid_indices),\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=6,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_idxs, val_idxs = get_train_val_idxs(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.core.decorators import auto_move_data\n",
    "from pytorch_lightning.metrics.functional.classification import auroc\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, sequence_length):\n",
    "        # returns embeds (sequence_length, 1, d_model)\n",
    "        return self.pe[:sequence_length, :]\n",
    "\n",
    "\n",
    "class RIIDDTransformerModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=0.001,\n",
    "        n_content_id=13943,  # number of different contents = 13942 + 1 (for padding)\n",
    "        n_part=8,  # number of different parts = 7 + 1 (for padding)\n",
    "        n_tags=189,  # number of different tags = 188 + 1 (for padding)\n",
    "        n_correct=5,  # 0,1 (false, true), 2 (start token), 3 (padding), 4 (lecture)\n",
    "        emb_dim=64,  # embedding dimension\n",
    "        dropout=0.1,\n",
    "        n_heads: int = 1,\n",
    "        n_encoder_layers: int = 2,\n",
    "        n_decoder_layers: int = 2,\n",
    "        dim_feedforward: int = 256,\n",
    "        activation: str = \"relu\",\n",
    "        max_window_size=100,\n",
    "        use_prior_q_times=False,\n",
    "        use_prior_q_explanation=False,\n",
    "    ):\n",
    "        super(RIIDDTransformerModel, self).__init__()\n",
    "        self.model_type = \"RiiidTransformer\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_window_size = max_window_size\n",
    "\n",
    "        self.use_prior_q_times = use_prior_q_times\n",
    "        self.use_prior_q_explanation = use_prior_q_explanation\n",
    "\n",
    "        # save params of models to yml\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        #### EXERCISE SEQUENCE\n",
    "        self.embed_content_id = nn.Embedding(n_content_id, emb_dim, padding_idx=13942)\n",
    "        self.embed_parts = nn.Embedding(n_part, emb_dim, padding_idx=0)\n",
    "        self.embed_tags = nn.Embedding(n_tags, emb_dim, padding_idx=188)\n",
    "\n",
    "        # exercise weights to weight the mean embeded excercise embeddings\n",
    "        self.exercise_weights = torch.nn.Parameter(torch.tensor([0.35, 0.55, 0.1]))\n",
    "\n",
    "        ### RESPONSE SEQUENCE (1st time stamp of sequence is useless)\n",
    "        self.embed_answered_correctly = nn.Embedding(\n",
    "            n_correct, emb_dim, padding_idx=3\n",
    "        )  # 2 + 1 for start token + 1 for padding_idn_inputs\n",
    "\n",
    "        self.embed_timestamps = nn.Linear(1, emb_dim)\n",
    "\n",
    "        # embed prior q time and q explanation\n",
    "        self.embed_prior_q_time = nn.Linear(1, emb_dim)\n",
    "        self.embed_prior_q_explanation = nn.Embedding(2, emb_dim)\n",
    "\n",
    "        # response weights to weight the mean embeded response embeddings\n",
    "        w = [0.5, 0.5]\n",
    "        if use_prior_q_times:\n",
    "            w.append(0.5)\n",
    "        if use_prior_q_explanation:\n",
    "            w.append(0.5)\n",
    "\n",
    "        self.response_weights = torch.nn.Parameter(torch.tensor([w]))\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=n_heads,\n",
    "            num_encoder_layers=n_encoder_layers,\n",
    "            num_decoder_layers=n_decoder_layers,\n",
    "            dropout=dropout,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "        self.out_linear = nn.Linear(emb_dim, 2)\n",
    "        init_weights(self)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = (\n",
    "            mask.float()\n",
    "            .masked_fill(mask == 0, float(\"-inf\"))\n",
    "            .masked_fill(mask == 1, float(0.0))\n",
    "        )\n",
    "        return mask\n",
    "\n",
    "    def get_random_steps(self, lengths, max_steps=10):\n",
    "        \"\"\"\n",
    "        for x return integer between 1 - 10 or\n",
    "        between 1 - x if x < 10\n",
    "        \"\"\"\n",
    "        m = torch.distributions.uniform.Uniform(\n",
    "            0,\n",
    "            (\n",
    "                torch.minimum(\n",
    "                    torch.ones(lengths.shape, device=self.device) * 10, lengths\n",
    "                )\n",
    "            ).float(),\n",
    "        )\n",
    "        return torch.floor(m.sample()).long() + 1\n",
    "\n",
    "    def get_random_lengths(self, lengths):\n",
    "        # gets random new lengths\n",
    "        m = torch.distributions.uniform.Uniform(0, lengths.float())\n",
    "        return torch.floor(m.sample()).long() + 1\n",
    "\n",
    "    def randomize_evaluation_step(self, batch, max_steps=10):\n",
    "        # randomize new lengths (for where start token is present)\n",
    "        # batch[\"length\"] = torch.where(\n",
    "        #     batch[\"answers\"][0, :] == 2,\n",
    "        #     self.get_random_lengths(batch[\"length\"]),\n",
    "        #     batch[\"length\"],\n",
    "        # )\n",
    "        # randomize number of steps based on new random lengths\n",
    "        batch[\"steps\"] = self.get_random_steps(batch[\"length\"], max_steps=max_steps)\n",
    "        return batch\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        content_ids,\n",
    "        parts,\n",
    "        answers,\n",
    "        tags,\n",
    "        timestamps,\n",
    "        prior_q_times,\n",
    "        prior_q_explanation=None,\n",
    "    ):\n",
    "        # content_ids: (Source Sequence Length, Number of samples, Embedding)\n",
    "        # tgt: (Target Sequence Length,Number of samples, Embedding)\n",
    "\n",
    "        # if data is flat then expand to get Batch dim\n",
    "        if len(content_ids.shape) == 1:\n",
    "            content_ids = content_ids.unsqueeze(1)\n",
    "            parts = parts.unsqueeze(1)\n",
    "            answers = answers.unsqueeze(1)\n",
    "            tags = tags.unsqueeze(1)\n",
    "            timestamps = timestamps.unsqueeze(1)\n",
    "            prior_q_times = prior_q_times.unsqueeze(1)\n",
    "            if prior_q_explanation is not None:\n",
    "                prior_q_explanation = prior_q_explanation.unsqueeze(1)\n",
    "\n",
    "        sequence_length = content_ids.shape[0]\n",
    "\n",
    "        # sequence that will go into encoder\n",
    "        embeded_content = self.embed_content_id(content_ids)\n",
    "        embeded_parts = self.embed_parts(parts)\n",
    "        embeded_tags = self.embed_tags(tags).sum(dim=2)\n",
    "        e_w = F.softmax(self.exercise_weights, dim=0)\n",
    "\n",
    "        embeded_exercise_sequence = (\n",
    "            torch.stack([embeded_content, embeded_parts, embeded_tags], dim=3) * e_w\n",
    "        ).sum(dim=3)\n",
    "\n",
    "        # sequence that will go into decoder\n",
    "        embeded_answered_correctly = self.embed_answered_correctly(answers)\n",
    "        embeded_timestamps = self.embed_timestamps(timestamps.unsqueeze(2))\n",
    "        r_w = F.softmax(self.response_weights, dim=0)\n",
    "\n",
    "        exercise_sequence_components = [embeded_answered_correctly, embeded_timestamps]\n",
    "        if self.use_prior_q_times:\n",
    "            embeded_q_times = self.embed_prior_q_time(prior_q_times.unsqueeze(2))\n",
    "            # zero embedding - if start token\n",
    "            embeded_q_times[0, torch.where(answers[0, :] == 2)[0], :] = 0\n",
    "            exercise_sequence_components.append(embeded_q_times)\n",
    "        if self.use_prior_q_explanation:\n",
    "            embeded_q_explanation = self.embed_prior_q_explanation(prior_q_explanation)\n",
    "            # zero embedding - if start token\n",
    "            embeded_q_explanation[0, torch.where(answers[0, :] == 2)[0], :] = 0\n",
    "            exercise_sequence_components.append(embeded_q_explanation)\n",
    "\n",
    "        embeded_responses = (\n",
    "            torch.stack(exercise_sequence_components, dim=3) * r_w\n",
    "        ).sum(dim=3)\n",
    "\n",
    "        # adding positional vector\n",
    "        embedded_positions = self.pos_encoder(sequence_length + 1)\n",
    "\n",
    "        # add shifted position embedding ( start token is first position)\n",
    "        embeded_responses = embeded_responses + embedded_positions[:-1, :, :]\n",
    "\n",
    "        embeded_exercise_sequence = (\n",
    "            embeded_exercise_sequence + embedded_positions[1:, :, :]\n",
    "        )\n",
    "\n",
    "        # mask of shape S x S -> prevents attention looking forward\n",
    "        top_right_attention_mask = self.generate_square_subsequent_mask(\n",
    "            sequence_length\n",
    "        ).type_as(embeded_exercise_sequence)\n",
    "\n",
    "        output = self.transformer(\n",
    "            embeded_exercise_sequence,\n",
    "            embeded_responses,\n",
    "            tgt_mask=top_right_attention_mask,  # (S,S)\n",
    "            src_mask=top_right_attention_mask,  # (T,T)\n",
    "        )\n",
    "\n",
    "        output = self.out_linear(output)\n",
    "        return F.softmax(output, dim=2)[:, :, 1]\n",
    "\n",
    "    def process_batch_step(self, batch):\n",
    "        return self(\n",
    "            batch[\"content_ids\"],\n",
    "            batch[\"parts\"],\n",
    "            batch[\"answers\"],\n",
    "            batch[\"tags\"],\n",
    "            batch[\"timestamps\"],\n",
    "            batch[\"prior_q_times\"],\n",
    "            batch[\"prior_q_explanation\"] if \"prior_q_explanation\" in batch else None,\n",
    "        )\n",
    "\n",
    "    @auto_move_data\n",
    "    def predict_n_steps(self, batch, steps, return_all_preds=False):\n",
    "        \"\"\"\n",
    "        Predicts n steps for all items in batch and return predictions\n",
    "        only for those steps (flattened)\n",
    "        steps: tensor of length B where each item is the number of steps that need to be taken\n",
    "        \"\"\"\n",
    "        seq_length, n_users = batch[\"content_ids\"].shape\n",
    "        lengths = batch[\"length\"]\n",
    "\n",
    "        users = torch.arange(n_users)\n",
    "\n",
    "        user_indexes = []\n",
    "        sequence_indexes = []\n",
    "\n",
    "        for i in range(steps.max().int(), 0, -1):\n",
    "            preds = self.process_batch_step(batch)\n",
    "\n",
    "            sequence_indexes_at_i = lengths[steps >= i] - i\n",
    "            user_indexes_at_i = users[steps >= i]\n",
    "\n",
    "            # get index for which to update the answers\n",
    "            # since answers is shifted we want to map preds 0..98 -> answers 1:99\n",
    "            answers_idx = torch.where(sequence_indexes_at_i + 1 != seq_length)\n",
    "            a_seq_idx = sequence_indexes_at_i[answers_idx] + 1\n",
    "            u_seq_idx = user_indexes_at_i[answers_idx]\n",
    "\n",
    "            # set answer to either 0 or 1 if not lecture\n",
    "            batch[\"answers\"][a_seq_idx, u_seq_idx] = torch.where(\n",
    "                batch[\"answers\"][a_seq_idx, u_seq_idx] != 4,\n",
    "                (preds[sequence_indexes_at_i[answers_idx], u_seq_idx] > 0.5).long(),\n",
    "                batch[\"answers\"][a_seq_idx, u_seq_idx],\n",
    "            )\n",
    "\n",
    "            user_indexes.append(user_indexes_at_i)\n",
    "            sequence_indexes.append(sequence_indexes_at_i)\n",
    "\n",
    "        if return_all_preds:\n",
    "            return preds\n",
    "\n",
    "        user_indexes = torch.cat(user_indexes)\n",
    "        sequence_indexes = torch.cat(sequence_indexes)\n",
    "\n",
    "        return (\n",
    "            preds[sequence_indexes, user_indexes],\n",
    "            batch[\"row_ids\"][sequence_indexes, user_indexes],\n",
    "        )\n",
    "\n",
    "    @auto_move_data\n",
    "    def predict_fast_single_user(\n",
    "        self,\n",
    "        content_ids,\n",
    "        parts,\n",
    "        answers,\n",
    "        tags,\n",
    "        timestamps,\n",
    "        prior_q_times,\n",
    "        prior_q_explanation=None,\n",
    "        n=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Predicts n steps for a single user in batch and return predictions\n",
    "        only for those steps (flattened)\n",
    "        \"\"\"\n",
    "        length = len(content_ids)\n",
    "        out_predictions = torch.zeros(n, device=self.device)\n",
    "        for i in range(n, 0, -1):\n",
    "            preds = self(\n",
    "                content_ids,\n",
    "                parts,\n",
    "                answers,\n",
    "                tags,\n",
    "                timestamps,\n",
    "                prior_q_times,\n",
    "                prior_q_explanation,\n",
    "            )\n",
    "            out_predictions[n - i] = preds[length - i, 0]\n",
    "\n",
    "            # answers are shifted (start token) so need + 1\n",
    "            answer_idx = length - i + 1\n",
    "            # don't update if at end of answers\n",
    "            if answer_idx + 1 < len(answers):\n",
    "                # don't update if true is lecture\n",
    "                if answers[answer_idx] != 4:\n",
    "                    answers[answer_idx] = (preds[length - i, 0] > 0.5).long()\n",
    "\n",
    "        return out_predictions\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        result = self.process_batch_step(batch)\n",
    "        loss = F.binary_cross_entropy(\n",
    "            result, batch[\"answered_correctly\"], weight=batch[\"loss_mask\"]\n",
    "        )\n",
    "        self.log(\"train_loss\", loss.cpu())\n",
    "        return loss\n",
    "\n",
    "    def validate_n_steps(self, batch, max_steps=10):\n",
    "        \"\"\"\n",
    "        Predicts max_steps steps for all items in batch and return predictions\n",
    "        only for those steps (flattened)\n",
    "        steps: tensor of length B where each item is the number of steps that need to be taken\n",
    "        \"\"\"\n",
    "        n_users = batch[\"content_ids\"].shape[1]\n",
    "        seq_length = batch[\"answers\"].shape[0]\n",
    "        lengths = batch[\"length\"]\n",
    "        steps = batch[\"steps\"]\n",
    "        users = torch.arange(n_users)\n",
    "        user_indexes = []\n",
    "        sequence_indexes = []\n",
    "        for i in range(steps.max().int(), 0, -1):\n",
    "            preds = self.process_batch_step(batch)\n",
    "            sequence_indexes_at_i = lengths[steps >= i] - i\n",
    "            user_indexes_at_i = users[steps >= i]\n",
    "\n",
    "            # get index for which to update the answers\n",
    "            # since answers is shifted we want to map preds 0..98 -> answers 1:99\n",
    "            answers_idx = torch.where(sequence_indexes_at_i + 1 != seq_length)\n",
    "            a_seq_idx = sequence_indexes_at_i[answers_idx] + 1\n",
    "            u_seq_idx = user_indexes_at_i[answers_idx]\n",
    "\n",
    "            # set answer to either 0 or 1 if not lecture\n",
    "            batch[\"answers\"][a_seq_idx, u_seq_idx] = torch.where(\n",
    "                batch[\"answers\"][a_seq_idx, u_seq_idx] != 4,\n",
    "                (preds[sequence_indexes_at_i[answers_idx], u_seq_idx] > 0.5).long(),\n",
    "                batch[\"answers\"][a_seq_idx, u_seq_idx],\n",
    "            )\n",
    "\n",
    "            user_indexes.append(user_indexes_at_i)\n",
    "            sequence_indexes.append(sequence_indexes_at_i)\n",
    "\n",
    "        user_indexes = torch.cat(user_indexes)\n",
    "        sequence_indexes = torch.cat(sequence_indexes)\n",
    "        return (preds, sequence_indexes, user_indexes)\n",
    "\n",
    "    def val_test_step(self, batch, log_as=\"val\"):\n",
    "        batch = self.randomize_evaluation_step(batch)\n",
    "\n",
    "        result, sequence_indexes, user_indexes = self.validate_n_steps(batch)\n",
    "\n",
    "        step_mask = torch.zeros(batch[\"loss_mask\"].shape, device=self.device)\n",
    "        step_mask[sequence_indexes, user_indexes] = 1\n",
    "\n",
    "        batch[\"loss_mask\"] *= step_mask\n",
    "\n",
    "        loss = F.binary_cross_entropy(\n",
    "            result, batch[\"answered_correctly\"], weight=batch[\"loss_mask\"]\n",
    "        )\n",
    "        self.log(f\"{log_as}_loss_step\", loss.cpu())\n",
    "        select_mask = batch[\"loss_mask\"] > 0\n",
    "        positions = torch.cat(\n",
    "            result.shape[1] * [torch.arange(result.shape[0]).unsqueeze(1)], dim=1\n",
    "        )\n",
    "        return (\n",
    "            torch.masked_select(result, select_mask),\n",
    "            torch.masked_select(batch[\"answered_correctly\"], select_mask),\n",
    "            torch.masked_select(positions, select_mask),\n",
    "        )\n",
    "\n",
    "    def val_test_epoch_end(self, outputs, log_as=\"val\", plot_acc=False):\n",
    "        y_pred = torch.cat([out[0] for out in outputs], dim=0).cpu()\n",
    "        y = torch.cat([out[1] for out in outputs], dim=0).cpu()\n",
    "        auc = auroc(y_pred, y)\n",
    "\n",
    "        if plot_acc:\n",
    "            pos = torch.cat([out[2] for out in outputs], dim=0)\n",
    "            # Calculate accuracy per position\n",
    "            M = torch.zeros(pos.max() + 1, len(y), device=self.device)\n",
    "            M[pos, torch.arange(len(y))] = 1\n",
    "            M = torch.nn.functional.normalize(M, p=1, dim=1)\n",
    "            acc_per_position = torch.mm(\n",
    "                M, ((y_pred > 0.5) == y).float().unsqueeze(1)\n",
    "            ).flatten()\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "            sns.regplot(\n",
    "                y=acc_per_position.cpu().numpy(),\n",
    "                x=torch.arange(len(acc_per_position)).cpu().numpy(),\n",
    "                ax=ax,\n",
    "            )\n",
    "            ax.set_ylim(0.5, 1)\n",
    "            ax.set_xlim(0, len(acc_per_position) - 1)\n",
    "            ax.set_ylabel(\"acc\")\n",
    "            ax.set_xlabel(\"position\")\n",
    "\n",
    "        if log_as == \"val\":\n",
    "            self.log(f\"avg_{log_as}_auc\", auc, prog_bar=True)\n",
    "            if plot_acc:\n",
    "                self.logger.experiment.add_figure(\n",
    "                    f\"{log_as}_acc_per_pos\", fig, global_step=self.current_epoch\n",
    "                )\n",
    "        else:\n",
    "            self.log(f\"avg_{log_as}_auc\", auc)\n",
    "            if plot_acc:\n",
    "                self.logger.experiment.add_figure(\n",
    "                    f\"{log_as}_acc_per_pos\", fig, global_step=1\n",
    "                )\n",
    "\n",
    "    def validation_step(self, batch, batch_nb, dataset_nb=None):\n",
    "        return self.val_test_step(batch, log_as=\"val\")\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.val_test_epoch_end(outputs, log_as=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_nb, dataset_nb=None):\n",
    "        return self.val_test_step(batch, log_as=\"test\")\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        self.val_test_epoch_end(outputs, log_as=\"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode=\"max\", patience=5\n",
    "            ),\n",
    "            \"monitor\": \"avg_val_auc\",\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 2500,\n",
    "            \"strict\": True,\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "\n",
    "# Params\n",
    "learning_rate = 0.004  # 0.0001\n",
    "emb_dim = 64  # 256\n",
    "dropout = 0.0\n",
    "n_heads = 2  # 2\n",
    "n_encoder_layers = 2\n",
    "n_decoder_layers = 2\n",
    "dim_feedforward = 256\n",
    "use_prior_q_times = False\n",
    "use_prior_q_explanation = False\n",
    "\n",
    "# Init our model\n",
    "model = RIIDDTransformerModel(\n",
    "    learning_rate=learning_rate,\n",
    "    emb_dim=emb_dim,  # embedding dimension - this is for everything\n",
    "    dropout=dropout,\n",
    "    n_heads=n_heads,\n",
    "    n_encoder_layers=n_encoder_layers,\n",
    "    n_decoder_layers=n_decoder_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    max_window_size=max_window_size,\n",
    "    use_prior_q_times=use_prior_q_times,\n",
    "    use_prior_q_explanation=use_prior_q_explanation,\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"base_no_lec\",)\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=500,\n",
    "    progress_bar_refresh_rate=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"avg_val_auc\", patience=20, mode=\"max\"),\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"avg_val_auc\",\n",
    "            filename=\"{epoch}-{val_loss_step:.2f}-{avg_val_auc:.2f}\",\n",
    "            mode=\"max\",\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ],\n",
    "    logger=logger,\n",
    "    val_check_interval=1000, # check validation every 1000 step\n",
    "    limit_val_batches=0.10, # run through only 25% of val every time\n",
    ")\n",
    "\n",
    "# Train the model ⚡\n",
    "# trainer.fit(\n",
    "#     model,\n",
    "#     train_dataloader=train_loader,\n",
    "#     val_dataloaders=[val_loader],\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best\n",
    "model = RIIDDTransformerModel.load_from_checkpoint(\n",
    "    \"nn-model/model.ckpt\"\n",
    ")\n",
    "\n",
    "model.freeze()\n",
    "\n",
    "# trainer.test(test_dataloaders=[val_loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.5000, 0.5000, 0.5000]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.response_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Kaggle Sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceRIIDDataset(Dataset):\n",
    "    \"\"\"RIID dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, hdf5_file=\"feats.h5\", window_size=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_mapping (np.array): array of all unique user ids \n",
    "            hdf5_file (string): location of hf5 feats file\n",
    "        \"\"\"\n",
    "        self.hdf5_file = hdf5_file\n",
    "        self.max_window_size = window_size\n",
    "        self.cache = {}\n",
    "\n",
    "    def open_hdf5(self):\n",
    "        # opens the h5py file\n",
    "        self.f = h5py.File(self.hdf5_file, \"r\")\n",
    "\n",
    "    def __len__(self):\n",
    "        if not hasattr(self, \"f\"):\n",
    "            self.open_hdf5()\n",
    "        return len(self.f.keys())\n",
    "\n",
    "    def load_user_into_cache(self, user_id):\n",
    "        \"\"\"\n",
    "        add a user to self.cache\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, \"f\"):\n",
    "            self.open_hdf5()\n",
    "\n",
    "        if f\"{user_id}\" in self.f:\n",
    "            content_ids = np.array(self.f[f\"{user_id}/content_ids\"], dtype=\"int64\")\n",
    "            answers = np.array(self.f[f\"{user_id}/answered_correctly\"], dtype=\"int64\")\n",
    "            timestamps = np.array(self.f[f\"{user_id}/timestamps\"], dtype=\"int64\")\n",
    "            prior_q_times = np.array(self.f[f\"{user_id}/prior_question_elapsed_time\"], dtype=\"float32\")\n",
    "\n",
    "            self.cache[user_id] = {\n",
    "                \"content_ids\": content_ids,\n",
    "                \"answers\": answers,\n",
    "                \"timestamps\": timestamps,\n",
    "                \"prior_q_times\": prior_q_times,\n",
    "                \"row_ids\": np.zeros(len(timestamps)),\n",
    "                \"steps\": 0,\n",
    "            }\n",
    "        else:\n",
    "            self.cache[user_id] = {\n",
    "                \"content_ids\": np.array([], dtype=\"int64\"),\n",
    "                \"timestamps\": np.array([], dtype=\"int64\"),\n",
    "                \"answers\": np.array([], dtype=\"int64\"),\n",
    "                \"prior_q_times\": np.array([], dtype=\"float32\"),\n",
    "                \"row_ids\": np.array([], dtype=\"int64\"),\n",
    "                \"steps\": 0,\n",
    "            }\n",
    "\n",
    "    def update_user_rows(self, user_rows):\n",
    "        if not hasattr(self, \"f\"):\n",
    "            self.open_hdf5()\n",
    "\n",
    "        user_id = user_rows.user_id.values[0]\n",
    "        num_rows = len(user_rows)\n",
    "        new_content_ids = user_rows.content_id.values\n",
    "        new_timestamps = user_rows.timestamp.values\n",
    "        new_answered_correctly = (\n",
    "            user_rows.answered_correctly.values\n",
    "        )  # should be 3 (3) for every question and 4 for lectures\n",
    "        new_prior_q_times = user_rows.prior_question_elapsed_time.values\n",
    "        \n",
    "        new_row_ids = user_rows.row_id.values\n",
    "\n",
    "        if user_id not in self.cache:\n",
    "            self.load_user_into_cache(user_id)\n",
    "\n",
    "        self.cache[user_id][\"content_ids\"] = np.concatenate(\n",
    "            [self.cache[user_id][\"content_ids\"], new_content_ids]\n",
    "        )[-(self.max_window_size + 1) :]\n",
    "\n",
    "        self.cache[user_id][\"timestamps\"] = np.concatenate(\n",
    "            [self.cache[user_id][\"timestamps\"], new_timestamps]\n",
    "        )[-(self.max_window_size + 1) :]\n",
    "\n",
    "        self.cache[user_id][\"answers\"] = np.concatenate(\n",
    "            [self.cache[user_id][\"answers\"], new_answered_correctly]\n",
    "        )[-(self.max_window_size + 1) :]\n",
    "\n",
    "        self.cache[user_id][\"prior_q_times\"] = np.concatenate(\n",
    "            [self.cache[user_id][\"prior_q_times\"], new_prior_q_times]\n",
    "        )[-(self.max_window_size + 1) :]\n",
    "\n",
    "        self.cache[user_id][\"row_ids\"] = np.concatenate(\n",
    "            [self.cache[user_id][\"row_ids\"], new_row_ids]\n",
    "        )[-(self.max_window_size + 1) :]\n",
    "\n",
    "        self.cache[user_id][\"steps\"] = len(new_row_ids)\n",
    "\n",
    "    def update_answered_correctly(self, answered_correctly_rows):\n",
    "        user_id = answered_correctly_rows.name\n",
    "        new_answered_correctly = (\n",
    "            answered_correctly_rows.values\n",
    "        )  # this is only the answers\n",
    "        \n",
    "        # update the correct answers for the questions (keep 4 - lectures)\n",
    "        n = self.cache[user_id][\"steps\"]\n",
    "        l = len(self.cache[user_id][\"answers\"])\n",
    "        idxs = np.where(self.cache[user_id][\"answers\"][-n:] != 4)[0] + l - n\n",
    "\n",
    "        self.cache[user_id][\"answers\"][idxs] = new_answered_correctly\n",
    "        self.cache[user_id][\"steps\"] = 0\n",
    "\n",
    "\n",
    "    def __getitem__(self, user_id):\n",
    "        if user_id not in self.cache:\n",
    "            self.load_user_into_cache(user_id)\n",
    "\n",
    "        length = len(self.cache[user_id][\"content_ids\"])\n",
    "        window_size = min(self.max_window_size, length)\n",
    "\n",
    "        content_ids = np.array(self.cache[user_id][\"content_ids\"][-window_size:]).copy()\n",
    "        answers = np.array(self.cache[user_id][\"answers\"][-window_size:]).copy()\n",
    "        timestamps = np.array(self.cache[user_id][\"timestamps\"][-window_size:]).copy()\n",
    "        prior_q_times = np.array(self.cache[user_id][\"prior_q_times\"][-window_size:]).copy()\n",
    "        row_ids = np.array(self.cache[user_id][\"row_ids\"][-window_size:]).copy()\n",
    "        \n",
    "        # convert timestamps to time elapsed\n",
    "        time_elapsed_timestamps = get_time_elapsed_from_timestamp(timestamps)\n",
    "\n",
    "        # get tags\n",
    "        tags = questions_lectures_tags[content_ids, :].astype(np.int64)\n",
    "\n",
    "        # get parts\n",
    "        parts = questions_lectures_parts[content_ids].astype(np.int64)\n",
    "        \n",
    "        # shift by one the answered_correctly sequence\n",
    "        answers = np.roll(answers, 1)\n",
    "\n",
    "\n",
    "        if length > self.max_window_size:\n",
    "            answers[0] = self.cache[user_id][\"answers\"][-window_size - 1]\n",
    "        else:\n",
    "            answers[0] = 2\n",
    "\n",
    "        return {\n",
    "            \"parts\": torch.from_numpy(parts).long(),\n",
    "            \"tags\": torch.from_numpy(tags).long(),\n",
    "            \"content_ids\": torch.from_numpy(content_ids).long(),\n",
    "            \"answers\": torch.from_numpy(answers).long(),\n",
    "            \"prior_q_times\": torch.from_numpy(prior_q_times).float(),\n",
    "            \"timestamps\": torch.from_numpy(time_elapsed_timestamps).float(),\n",
    "            \"length\": window_size,\n",
    "            \"row_ids\": torch.from_numpy(row_ids).int(),\n",
    "            \"steps\": self.cache[user_id][\"steps\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# The collate function is used to merge individual data samples into a batch\n",
    "# It handles the padding aspect\n",
    "def inference_collate_fn(batch):\n",
    "    # collate lenghts into 1D tensor\n",
    "    items = {\n",
    "        \"length\": torch.tensor([batch_item[\"length\"] for batch_item in batch]),\n",
    "        \"steps\": torch.tensor([batch_item[\"steps\"] for batch_item in batch]),\n",
    "    }\n",
    "\n",
    "    # find shape that the batch will have\n",
    "    max_length = items[\"length\"].max()\n",
    "    num_items = len(batch)\n",
    "    if num_items > 1:\n",
    "        # padding list\n",
    "        for (key, padding) in [\n",
    "            (\"parts\", 0),\n",
    "            (\"content_ids\", 13942),\n",
    "            (\"answers\", 3),\n",
    "            (\"timestamps\", 0.0),  # note timestamps isnt an embedding\n",
    "            (\"prior_q_times\", 0),\n",
    "            (\"tags\", 188),\n",
    "            (\"row_ids\", 0),\n",
    "        ]:\n",
    "            items[key] = pad_sequence(\n",
    "                [batch_item[key] for batch_item in batch],\n",
    "                batch_first=False,\n",
    "                padding_value=padding,\n",
    "            )\n",
    "    else:\n",
    "        for key in [\n",
    "            \"parts\",\n",
    "            \"content_ids\",\n",
    "            \"answers\",\n",
    "            \"timestamps\",\n",
    "            \"tags\",\n",
    "            \"row_ids\",\n",
    "            \"prior_q_times\",\n",
    "        ]:\n",
    "            items[key] = batch[0][key].unsqueeze(1)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "model = RIIDDTransformerModel.load_from_checkpoint(\"nn-model/model.ckpt\")\n",
    "model.freeze()\n",
    "model.cuda()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "cv_valid_ids = pd.read_pickle(f\"cv5_valid.pickle\")[\"row_id\"].values\n",
    "valid = train[train.row_id.isin(cv_valid_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 477 ms, total: 1min\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_groups = np.array_split(valid, len(valid) // 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = InferenceRIIDDataset(hdf5_file=\"train_no_lec_feats.h5\",)\n",
    "\n",
    "\n",
    "def predict_for_df(df):\n",
    "    # select questions only\n",
    "    unique_users_with_questions = df[~df.content_type_id].user_id.unique()\n",
    "    df[\"answered_correctly\"] = 0.5  # set useless value for column\n",
    "\n",
    "    # batch with only lectures\n",
    "    if len(unique_users_with_questions) < 1:\n",
    "        pass  # return empy\n",
    "\n",
    "    # case of single user with questions in batch\n",
    "    elif len(unique_users_with_questions) == 1:\n",
    "        item = inference_dataset[unique_users_with_questions[0]].copy()\n",
    "        predictions = model.predict_fast_single_user(\n",
    "            item[\"content_ids\"],\n",
    "            item[\"parts\"],\n",
    "            item[\"answers\"],\n",
    "            item[\"tags\"],\n",
    "            item[\"timestamps\"],\n",
    "            item[\"prior_q_times\"],\n",
    "            n=item[\"steps\"],\n",
    "        ).cpu()\n",
    "        df.loc[\n",
    "            df.user_id == unique_users_with_questions[0], \"answered_correctly\"\n",
    "        ] = predictions.numpy()\n",
    "    # case of multiple users with questions in batch\n",
    "    else:\n",
    "        batch = inference_collate_fn(\n",
    "            [inference_dataset[u].copy() for u in unique_users_with_questions]\n",
    "        )\n",
    "        predictions, row_ids = model.predict_n_steps(batch, batch[\"steps\"])\n",
    "        df[\"answered_correctly\"] = df[\"row_id\"].map(\n",
    "            dict(zip(row_ids.cpu().numpy(), predictions.cpu().numpy()))\n",
    "        )\n",
    "    return df[~df.content_type_id]\n",
    "\n",
    "\n",
    "previous_test_df = None\n",
    "predictions = []\n",
    "row_ids = []\n",
    "for current_test in tqdm(valid_groups):\n",
    "    if previous_test_df is not None:\n",
    "        previous_test_df[~previous_test_df.content_type_id].groupby(\n",
    "            \"user_id\"\n",
    "        ).answered_correctly.apply(inference_dataset.update_answered_correctly)\n",
    "    # add current to cache\n",
    "    current_test = current_test.copy()\n",
    "    previous_test_df = current_test.copy()\n",
    "\n",
    "    current_test.drop(columns=[\"answered_correctly\"], inplace=True)\n",
    "    current_test =  current_test[~current_test.content_type_id].copy()\n",
    "\n",
    "    # preprocessing code heree\n",
    "    current_test = preprocess_df(current_test)\n",
    "    current_test[\n",
    "        [\"row_id\", \"user_id\", \"content_id\", \"timestamp\", \"answered_correctly\", \"prior_question_elapsed_time\"]\n",
    "    ].groupby(\"user_id\").apply(\n",
    "        lambda user_rows: inference_dataset.update_user_rows(user_rows)\n",
    "    )\n",
    "\n",
    "    # your prediction code here\n",
    "    current_test = predict_for_df(current_test)\n",
    "    predictions.append(current_test[\"answered_correctly\"].values)\n",
    "    row_ids.append(current_test[\"row_id\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation auc: 0.7889184570559918\n"
     ]
    }
   ],
   "source": [
    "all_preds = np.concatenate(predictions)\n",
    "all_row_ids = np.concatenate(row_ids)\n",
    "df = pd.DataFrame({\"predictions\": all_preds, \"row_id\": all_row_ids})\n",
    "df = df.merge(valid[[\"row_id\", \"user_id\", \"answered_correctly\"]], on=\"row_id\", how=\"left\")\n",
    "print('validation auc:',roc_auc_score(df.answered_correctly.values, df.predictions.values))\n",
    "\n",
    "#validation auc: 0.7704724230724678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def roc_score_per_interaction(x):\n",
    "    try:\n",
    "        return roc_auc_score(x.answered_correctly.values, x.predictions.values)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df[\"num_iteractions\"] = (df.groupby(\"user_id\").cumcount() // 10) * 10\n",
    "interactions_auc = df.groupby(\"num_iteractions\").apply(roc_score_per_interaction)\n",
    "interactions_auc = interactions_auc[~interactions_auc.isna()].copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.regplot(\n",
    "    y=interactions_auc.values,\n",
    "    x=interactions_auc.index,\n",
    "    line_kws={\"color\": \"red\", \"linewidth\": 2},\n",
    "    scatter_kws={\"s\": 3},\n",
    ")\n",
    "ax.set_ylim(0.5, 1)\n",
    "ax.set_xlim(0, interactions_auc.index.max())\n",
    "ax.set_ylabel(\"auc\")\n",
    "ax.set_xlabel(\"number of seen interactions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Iter_Valid(object):\n",
    "    def __init__(self, df, max_user=1000):\n",
    "        df = df.reset_index(drop=True)\n",
    "        self.df = df\n",
    "        self.user_answer = df[\"user_answer\"].astype(str).values\n",
    "        self.answered_correctly = df[\"answered_correctly\"].astype(str).values\n",
    "        df[\"prior_group_responses\"] = \"[]\"\n",
    "        df[\"prior_group_answers_correct\"] = \"[]\"\n",
    "        self.sample_df = df[df[\"content_type_id\"] == 0][[\"row_id\"]]\n",
    "        self.sample_df[\"answered_correctly\"] = 0\n",
    "        self.len = len(df)\n",
    "        self.user_id = df.user_id.values\n",
    "        self.task_container_id = df.task_container_id.values\n",
    "        self.content_type_id = df.content_type_id.values\n",
    "        self.max_user = max_user\n",
    "        self.current = 0\n",
    "        self.pre_user_answer_list = []\n",
    "        self.pre_answered_correctly_list = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def fix_df(self, user_answer_list, answered_correctly_list, pre_start):\n",
    "        df = self.df[pre_start : self.current].copy()\n",
    "        sample_df = self.sample_df[pre_start : self.current].copy()\n",
    "        df.loc[pre_start, \"prior_group_responses\"] = (\n",
    "            \"[\" + \",\".join(self.pre_user_answer_list) + \"]\"\n",
    "        )\n",
    "        df.loc[pre_start, \"prior_group_answers_correct\"] = (\n",
    "            \"[\" + \",\".join(self.pre_answered_correctly_list) + \"]\"\n",
    "        )\n",
    "        self.pre_user_answer_list = user_answer_list\n",
    "        self.pre_answered_correctly_list = answered_correctly_list\n",
    "        return df, sample_df\n",
    "\n",
    "    def __next__(self):\n",
    "        added_user = set()\n",
    "        pre_start = self.current\n",
    "        pre_added_user = -1\n",
    "        pre_task_container_id = -1\n",
    "\n",
    "        user_answer_list = []\n",
    "        answered_correctly_list = []\n",
    "        while self.current < self.len:\n",
    "            crr_user_id = self.user_id[self.current]\n",
    "            crr_task_container_id = self.task_container_id[self.current]\n",
    "            crr_content_type_id = self.content_type_id[self.current]\n",
    "            if crr_content_type_id == 1:\n",
    "                # no more than one task_container_id of \"questions\" from any single user\n",
    "                # so we only care for content_type_id == 0 to break loop\n",
    "                user_answer_list.append(self.user_answer[self.current])\n",
    "                answered_correctly_list.append(self.answered_correctly[self.current])\n",
    "                self.current += 1\n",
    "                continue\n",
    "            if crr_user_id in added_user and (\n",
    "                (crr_user_id != pre_added_user)\n",
    "                or (crr_task_container_id != pre_task_container_id)\n",
    "            ):\n",
    "                # known user(not prev user or differnt task container)\n",
    "                return self.fix_df(user_answer_list, answered_correctly_list, pre_start)\n",
    "            if len(added_user) == self.max_user:\n",
    "                if (\n",
    "                    crr_user_id == pre_added_user\n",
    "                    and crr_task_container_id == pre_task_container_id\n",
    "                ):\n",
    "                    user_answer_list.append(self.user_answer[self.current])\n",
    "                    answered_correctly_list.append(\n",
    "                        self.answered_correctly[self.current]\n",
    "                    )\n",
    "                    self.current += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    return self.fix_df(\n",
    "                        user_answer_list, answered_correctly_list, pre_start\n",
    "                    )\n",
    "            added_user.add(crr_user_id)\n",
    "            pre_added_user = crr_user_id\n",
    "            pre_task_container_id = crr_task_container_id\n",
    "            user_answer_list.append(self.user_answer[self.current])\n",
    "            answered_correctly_list.append(self.answered_correctly[self.current])\n",
    "            self.current += 1\n",
    "        if pre_start < self.current:\n",
    "            return self.fix_df(user_answer_list, answered_correctly_list, pre_start)\n",
    "        else:\n",
    "            raise StopIteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.21 s, sys: 96.8 ms, total: 2.3 s\n",
      "Wall time: 2.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inference_dataset = InferenceRIIDDataset(hdf5_file=\"train_no_lec_feats.h5\",)\n",
    "iter_test = Iter_Valid(valid,max_user=1000)\n",
    "predicted = []\n",
    "def set_predict(df):\n",
    "    predicted.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a861107212472194da78c1f64112ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2500000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-498d6386a308>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# your feature extraction and model training code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mcurrent_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprevious_test_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c760b0aa49b8>\u001b[0m in \u001b[0;36mpreprocess_df\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# map lecture ids to new content_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     df.loc[df.content_type_id, \"content_id\"] = df[df.content_type_id].content_id.map(\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mlectures_mapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     )\n",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   3628\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3629\u001b[0m         \"\"\"\n\u001b[0;32m-> 3630\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3631\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.8/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;31m# of dtype float64 the return value of this method should\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m                 \u001b[0;31m# be float64 as well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m                 mapper = create_series_with_explicit_dtype(\n\u001b[0m\u001b[1;32m   1106\u001b[0m                     \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_if_empty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 )\n",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.8/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36mcreate_series_with_explicit_dtype\u001b[0;34m(data, index, dtype, name, copy, fastpath, dtype_if_empty)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_empty_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype_if_empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m     return Series(\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfastpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     )\n",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m                 \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, dtype)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;31m# TODO: passing np.float64 to not break anything yet. See GH-17261\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         s = create_series_with_explicit_dtype(\n\u001b[0m\u001b[1;32m    349\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_if_empty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.8/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36mcreate_series_with_explicit_dtype\u001b[0;34m(data, index, dtype, name, copy, fastpath, dtype_if_empty)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_empty_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype_if_empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m     return Series(\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfastpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     )\n",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   5355\u001b[0m             \u001b[0mindex_like\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5357\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;31m# other iterable of some kind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \"\"\"\n",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_cast_data_without_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                     return cls(\n",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_maybe_cast_data_without_dtype\u001b[0;34m(subarr)\u001b[0m\n\u001b[1;32m   5487\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minferred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"integer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5488\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5489\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_convert_to_int_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5490\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5491\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_try_convert_to_int_array\u001b[0;34m(data, copy, dtype)\u001b[0m\n\u001b[1;32m   5559\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5560\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5561\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5562\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m  \u001b[0;31m# TODO: might still need to copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOverflowError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inference_dataset = InferenceRIIDDataset(hdf5_file=\"train_no_lec_feats.h5\",)\n",
    "iter_test = Iter_Valid(valid,max_user=1000)\n",
    "predicted = []\n",
    "def set_predict(df):\n",
    "    predicted.append(df)\n",
    "\n",
    "pbar = tqdm(total=len(valid))\n",
    "previous_test_df = None\n",
    "for (current_test, current_prediction_df) in iter_test:\n",
    "    if previous_test_df is not None:\n",
    "        previous_test_df[\"answered_correctly\"] = eval(current_test[\"prior_group_answers_correct\"].iloc[0])\n",
    "        previous_test_df[~previous_test_df.content_type_id].groupby(\n",
    "            \"user_id\"\n",
    "        ).answered_correctly.apply(inference_dataset.update_answered_correctly)\n",
    "        \n",
    "\n",
    "    # your feature extraction and model training code here\n",
    "    current_test = preprocess_df(current_test)\n",
    "    previous_test_df = current_test.copy()\n",
    "    \n",
    "    current_test = current_test[~current_test.content_type_id].copy()\n",
    "\n",
    "    # add current to cache\n",
    "    current_test[\n",
    "        [\"row_id\", \"user_id\", \"content_id\", \"timestamp\", \"answered_correctly\", \"prior_question_elapsed_time\"]\n",
    "    ].groupby(\"user_id\").apply(\n",
    "        lambda user_rows: inference_dataset.update_user_rows(user_rows)\n",
    "    )\n",
    "    \n",
    "    # your prediction code here\n",
    "    current_test = predict_for_df(current_test)\n",
    "    set_predict(current_test.loc[:, [\"row_id\", \"answered_correctly\"]])\n",
    "    pbar.update(len(current_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation auc: 0.8077434878428923\n"
     ]
    }
   ],
   "source": [
    "#validation score\n",
    "y_pred = pd.concat(predicted).answered_correctly\n",
    "y_true = valid[valid.content_type_id == 0].answered_correctly[:len(y_pred)]\n",
    "\n",
    "print('validation auc:',roc_auc_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10977      1\n",
       "10978      1\n",
       "10979      1\n",
       "10980      1\n",
       "10981      1\n",
       "          ..\n",
       "4292693    1\n",
       "4292694    0\n",
       "4292695    1\n",
       "4292696    1\n",
       "4292697    1\n",
       "Name: answered_correctly, Length: 115947, dtype: int8"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.635927\n",
       "1         0.588397\n",
       "2         0.581650\n",
       "3         0.911703\n",
       "4         0.737461\n",
       "            ...   \n",
       "118026    0.956787\n",
       "118027    0.911273\n",
       "118028    0.783240\n",
       "118029    0.995036\n",
       "118030    0.741667\n",
       "Name: answered_correctly, Length: 115947, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
