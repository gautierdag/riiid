{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.style as style\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "style.use(\"fivethirtyeight\")\n",
    "import gc\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "import lightgbm as lgb\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "from pytorch_lightning import seed_everything\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "\n",
    "SEED = 69\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lectures arrays\n",
      "Loading questions arrays\n",
      "CPU times: user 22 ms, sys: 1.06 s, total: 1.08 s\n",
      "Wall time: 1.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This code is needed for dataset\n",
    "train = pd.read_pickle(\"riiid_train.pkl.gzip\")\n",
    "questions_df = pd.read_csv(\"questions.csv\")\n",
    "lectures_df = pd.read_csv(\"lectures.csv\")\n",
    "\n",
    "folder_path = \"data\"\n",
    "print(\"Loading lectures arrays\")\n",
    "lectures_ids = np.load(f\"{folder_path}/lectures_ids.npy\")\n",
    "lectures_parts = np.load(f\"{folder_path}/lectures_parts.npy\")\n",
    "lectures_types = np.load(f\"{folder_path}/lectures_types.npy\")\n",
    "lectures_tags = lectures_df.tag.values \n",
    "\n",
    "print(\"Loading questions arrays\")\n",
    "questions_parts = np.load(f\"{folder_path}/questions_parts.npy\")\n",
    "\n",
    "# process tags\n",
    "def split_tags(t):\n",
    "    try:\n",
    "        return [int(i) for i in t.split(\" \")]\n",
    "    except AttributeError:\n",
    "        return list()\n",
    "\n",
    "# Get tags to be 2D array of shape (Q, T), where Q is question_idx, and T is the max number of tag possible (6)\n",
    "questions_df[\"tags\"] = questions_df.tags.apply(split_tags)\n",
    "questions_tags = pd.DataFrame(questions_df[\"tags\"].tolist(), index=questions_df.index)\n",
    "questions_tags = questions_tags.fillna(questions_tags.max().max()+1).astype(np.int).values # pad with max tag + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_train = pd.read_pickle(f\"cv5_train.pickle\")[\"row_id\"]\n",
    "cv_valid = pd.read_pickle(f\"cv5_valid.pickle\")[\"row_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.5 s, sys: 7 s, total: 25.5 s\n",
      "Wall time: 25.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23551"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "valid = train[train.row_id.isin(cv_valid)].copy()\n",
    "train = train[train.row_id.isin(cv_train)].copy()\n",
    "del cv_valid, cv_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_parts_mapping = dict(zip(np.arange(len(questions_parts)), questions_parts))\n",
    "lectures_parts_mapping = dict(\n",
    "    zip(lectures_df[\"lecture_id\"].values, lectures_df[\"part\"].values)\n",
    ")\n",
    "\n",
    "# Add part to lecture and questions\n",
    "def add_part_to_df(df):\n",
    "    df[\"part\"] = 0\n",
    "    df.loc[~df.content_type_id, \"part\"] = df[~df.content_type_id].content_id.map(\n",
    "        questions_parts_mapping\n",
    "    )\n",
    "    df.loc[df.content_type_id, \"part\"] = df[df.content_type_id].content_id.map(\n",
    "        lectures_parts_mapping\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_part_to_questions(df):\n",
    "    df[\"part\"] = df.content_id.map(questions_parts_mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_elapsed_from_timestamp(arr, max_minutes=300):\n",
    "    # Saint+ way\n",
    "    # Note this is isnt the smartest way..\n",
    "    arr_seconds = np.diff(arr, prepend=1) // 1000\n",
    "    arr_seconds[arr_seconds > max_minutes] = max_minutes\n",
    "    return arr_seconds.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Hdf5\n",
    "\n",
    "Only run this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_part_to_df(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['row_id', 'timestamp', 'user_id', 'content_id', 'content_type_id',\n",
       "       'task_container_id', 'user_answer', 'answered_correctly',\n",
       "       'prior_question_elapsed_time', 'prior_question_had_explanation',\n",
       "       'part'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore lectures for now\n",
    "hf = h5py.File(\"feat_train.h5\", \"w\")\n",
    "\n",
    "for user_id, data in tqdm(train[~train.content_type_id].groupby(\"user_id\")):\n",
    "    processed_feats = data[\n",
    "        [\n",
    "            \"content_id\",\n",
    "            \"part\",\n",
    "            \"answered_correctly\",\n",
    "            \"timestamp\",\n",
    "        ]\n",
    "    ].values\n",
    "\n",
    "    hf.create_dataset(f\"{user_id}/content_ids\", data=processed_feats[:, 0], maxshape=(None,))\n",
    "    hf.create_dataset(f\"{user_id}/parts\", data=processed_feats[:, 1], maxshape=(None,))\n",
    "    hf.create_dataset(f\"{user_id}/answered_correctly\", data=processed_feats[:, 2], maxshape=(None,))\n",
    "    hf.create_dataset(f\"{user_id}/timestamps\", data=processed_feats[:, 3], maxshape=(None,))\n",
    "\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Here we define the pytorch Dataset object and a custom collate function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RIIDDataset(Dataset):\n",
    "    \"\"\"RIID dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, user_mapping, hf5_file=\"feats_train.h5\", window_size=WINDOW_SIZE,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_mapping (np.array): array of all unique user ids \n",
    "            hf5_file (string): location of hf5 feats file\n",
    "        \"\"\"\n",
    "        # np array where index maps to a user id\n",
    "        self.user_mapping = user_mapping\n",
    "        self.hf5_file = hf5_file\n",
    "        self.max_window_size = window_size\n",
    "\n",
    "    def open_hdf5(self):\n",
    "        # opens the h5py file\n",
    "        self.f = h5py.File(self.hf5_file, \"r\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_mapping)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # open the hdf5 file in the iterator to allow multiple workers\n",
    "        # https://github.com/pytorch/pytorch/issues/11929\n",
    "        if not hasattr(self, \"f\"):\n",
    "            self.open_hdf5()\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        user_id = self.user_mapping[idx]\n",
    "        length = self.f[f\"{user_id}/answered_correctly\"].len()\n",
    "\n",
    "        window_size = min(self.max_window_size, length)\n",
    "\n",
    "        parts = np.zeros(window_size, dtype=np.int64).copy()\n",
    "        content_ids = np.zeros(window_size, dtype=np.int64).copy()\n",
    "        answered_correctly = np.zeros(window_size, dtype=np.int64).copy()\n",
    "        timestamps = np.zeros(window_size, dtype=np.float32).copy()\n",
    "\n",
    "        # index for loading larger than window size\n",
    "        start_index = 0\n",
    "        if length > window_size:\n",
    "            # randomly select window size subset instead of trying to cram in everything\n",
    "            start_index = np.random.randint(length - window_size)\n",
    "\n",
    "        self.f[f\"{user_id}/content_ids\"].read_direct(\n",
    "            content_ids,\n",
    "            source_sel=np.s_[start_index : start_index + window_size],\n",
    "            dest_sel=np.s_[0:window_size],\n",
    "        )\n",
    "        self.f[f\"{user_id}/parts\"].read_direct(\n",
    "            parts,\n",
    "            source_sel=np.s_[start_index : start_index + window_size],\n",
    "            dest_sel=np.s_[0:window_size],\n",
    "        )\n",
    "        self.f[f\"{user_id}/answered_correctly\"].read_direct(\n",
    "            answered_correctly,\n",
    "            source_sel=np.s_[start_index : start_index + window_size],\n",
    "            dest_sel=np.s_[0:window_size],\n",
    "        )\n",
    "        self.f[f\"{user_id}/timestamps\"].read_direct(\n",
    "            timestamps,\n",
    "            source_sel=np.s_[start_index : start_index + window_size],\n",
    "            dest_sel=np.s_[0:window_size],\n",
    "        )\n",
    "\n",
    "        # convert timestamps to time elapsed\n",
    "        time_elapsed_timestamps = get_time_elapsed_from_timestamp(timestamps)\n",
    "\n",
    "        # get question tags\n",
    "        tags = questions_tags[content_ids, :].astype(np.int64)\n",
    "\n",
    "        # shift by one the answered_correctly sequence\n",
    "        answers = np.roll(answered_correctly, 1)\n",
    "\n",
    "        # set start token if start_index is actually first element\n",
    "        if start_index == 0:\n",
    "            answers[0] = 2\n",
    "        # else replace first element of sequence with actual previous element\n",
    "        else: \n",
    "            self.f[f\"{user_id}/answered_correctly\"].read_direct(\n",
    "                answers,\n",
    "                source_sel=np.s_[start_index-1],\n",
    "                dest_sel=np.s_[0],\n",
    "            )\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \"parts\": torch.from_numpy(parts),\n",
    "            \"tags\": torch.from_numpy(tags),\n",
    "            \"content_ids\": torch.from_numpy(content_ids),\n",
    "            \"answered_correctly\": torch.from_numpy(answered_correctly),\n",
    "            \"answers\": torch.from_numpy(answers),\n",
    "            \"timestamps\": torch.from_numpy(time_elapsed_timestamps),\n",
    "            \"length\": window_size,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# The collate function is used to merge individual data samples into a batch\n",
    "# It handles the padding aspect\n",
    "def collate_fn(batch):\n",
    "    # collate lenghts into 1D tensor\n",
    "    items = {\"length\": torch.tensor([batch_item[\"length\"] for batch_item in batch])}\n",
    "\n",
    "    # find shape that the batch will have\n",
    "    max_length = items[\"length\"].max()\n",
    "    num_items = len(batch)\n",
    "\n",
    "    # padding list\n",
    "    for (key, padding) in [\n",
    "        (\"parts\", 0),\n",
    "        (\"content_ids\", 13523),\n",
    "        (\"answered_correctly\", 3),\n",
    "        (\"answers\", 3),\n",
    "        (\"timestamps\", 0.0), # note timestamps isnt an embedding\n",
    "        (\"tags\", 188),\n",
    "    ]:\n",
    "        items[key] = pad_sequence(\n",
    "            [batch_item[key] for batch_item in batch],\n",
    "            batch_first=False,\n",
    "            padding_value=padding,\n",
    "        )\n",
    "\n",
    "    # mask to weight loss by (S, N)\n",
    "    items[\"loss_mask\"] = (\n",
    "        (\n",
    "            torch.arange(max_length).expand(num_items, max_length)\n",
    "            < items[\"length\"].unsqueeze(1)\n",
    "        )\n",
    "        .transpose(1, 0)\n",
    "        .float()\n",
    "    )\n",
    "\n",
    "    items[\"answered_correctly\"] = items[\"answered_correctly\"].float()\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393656"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dataset will all users\n",
    "user_ids = train.user_id.unique()\n",
    "dataset = RIIDDataset(user_ids,hf5_file=\"nn-data/feats.h5\", window_size=WINDOW_SIZE)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.core.decorators import auto_move_data\n",
    "from pytorch_lightning.metrics.functional.classification import auroc\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, sequence_length):\n",
    "        # returns embeds (sequence_length, 1, d_model)\n",
    "        return self.pe[:sequence_length, :]\n",
    "\n",
    "\n",
    "class RIIDDTransformerModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=0.001,\n",
    "        n_content_id=13524,  # number of different contents = 13523 + 1 (for padding)\n",
    "        n_part=8,  # number of different parts = 7 + 1 (for padding)\n",
    "        n_tags=189,  # number of different tags = 188 + 1 (for padding)\n",
    "        emb_dim=64,  # embedding dimension\n",
    "        dropout=0.1,\n",
    "        n_heads: int = 1,\n",
    "        n_encoder_layers: int = 2,\n",
    "        n_decoder_layers: int = 2,\n",
    "        dim_feedforward: int = 256,\n",
    "        activation: str = \"relu\",\n",
    "        batch_size=256,  # will get saved as hyperparam\n",
    "        num_user_train=300000,  # will get saved as hyperparam\n",
    "        num_user_val=30000,  # will get saved as hyperparam\n",
    "        max_window_size=100,\n",
    "        validate_on_sequence=True,\n",
    "    ):\n",
    "        super(RIIDDTransformerModel, self).__init__()\n",
    "        self.model_type = \"RiiidTransformer\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_window_size = max_window_size\n",
    "        self.validate_on_sequence = validate_on_sequence\n",
    "\n",
    "        # save params of models to yml\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.embed_content_id = nn.Embedding(n_content_id, emb_dim, padding_idx=13523)\n",
    "        self.embed_parts = nn.Embedding(n_part, emb_dim, padding_idx=0)\n",
    "        self.embed_tags = nn.Embedding(n_tags, emb_dim, padding_idx=188)\n",
    "        # exercise weights to weight the mean embeded excercise embeddings\n",
    "        self.exercise_weights = torch.nn.Parameter(torch.tensor([0.35, 0.55, 0.1]))\n",
    "\n",
    "        self.embed_answered_correctly = nn.Embedding(\n",
    "            4, emb_dim, padding_idx=3\n",
    "        )  # 2 + 1 for start token + 1 for padding_idn_inputs\n",
    "\n",
    "        self.embed_timestamps = nn.Linear(1, emb_dim)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=n_heads,\n",
    "            num_encoder_layers=n_encoder_layers,\n",
    "            num_decoder_layers=n_decoder_layers,\n",
    "            dropout=dropout,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "        self.out_linear = nn.Linear(emb_dim, 2)\n",
    "        init_weights(self)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = (\n",
    "            mask.float()\n",
    "            .masked_fill(mask == 0, float(\"-inf\"))\n",
    "            .masked_fill(mask == 1, float(0.0))\n",
    "        )\n",
    "        return mask\n",
    "\n",
    "    @auto_move_data\n",
    "    def forward(self, content_ids, parts, answers, tags, timestamps):\n",
    "        # content_ids: (Source Sequence Length, Number of samples, Embedding)\n",
    "        # tgt: (Target Sequence Length,Number of samples, Embedding)\n",
    "\n",
    "        # if data is flat then expand to get Batch dim\n",
    "        if len(content_ids.shape) == 1:\n",
    "            content_ids = content_ids.unsqueeze(1)\n",
    "            parts = parts.unsqueeze(1)\n",
    "            answers = answers.unsqueeze(1)\n",
    "            tags = tags.unsqueeze(1)\n",
    "            timestamps = timestamps.unsqueeze(1)\n",
    "\n",
    "        sequence_length = content_ids.shape[0]\n",
    "\n",
    "        # sequence that will go into encoder\n",
    "        embeded_content = self.embed_content_id(content_ids)\n",
    "        embeded_parts = self.embed_parts(parts)\n",
    "        embeded_tags = self.embed_tags(tags).sum(dim=2)\n",
    "        e_w = F.softmax(self.exercise_weights, dim=0)\n",
    "\n",
    "        embeded_exercise_sequence = (\n",
    "            (embeded_content * e_w[0])\n",
    "            + (embeded_parts * e_w[1])\n",
    "            + (embeded_tags * e_w[2])\n",
    "        )\n",
    "\n",
    "        # sequence that will go into decoder\n",
    "        embeded_responses = self.embed_answered_correctly(answers)\n",
    "        embeded_timestamps = self.embed_timestamps(timestamps.unsqueeze(2))\n",
    "        embeded_responses = (embeded_responses + embeded_timestamps) * 0.5\n",
    "\n",
    "        # adding positional vector\n",
    "        embedded_positions = self.pos_encoder(sequence_length)\n",
    "        embeded_responses = embeded_responses + embedded_positions\n",
    "        embeded_exercise_sequence = embeded_exercise_sequence + embedded_positions\n",
    "\n",
    "        # mask of shape S x S -> prevents attention looking forward\n",
    "        top_right_attention_mask = self.generate_square_subsequent_mask(\n",
    "            sequence_length\n",
    "        ).type_as(embeded_exercise_sequence)\n",
    "\n",
    "        output = self.transformer(\n",
    "            embeded_exercise_sequence,\n",
    "            embeded_responses,\n",
    "            tgt_mask=top_right_attention_mask,  # (S,S)\n",
    "            src_mask=top_right_attention_mask,  # (T,T)\n",
    "        )\n",
    "\n",
    "        output = self.out_linear(output)\n",
    "        return F.softmax(output, dim=2)[:, :, 1]\n",
    "\n",
    "    def process_batch_step(self, batch):\n",
    "        # return result\n",
    "        return self(\n",
    "            batch[\"content_ids\"],\n",
    "            batch[\"parts\"],\n",
    "            batch[\"answers\"],\n",
    "            batch[\"tags\"],\n",
    "            batch[\"timestamps\"],\n",
    "        )\n",
    "\n",
    "    @auto_move_data\n",
    "    def predict_n_steps(self, batch, steps, return_all_preds=False):\n",
    "        \"\"\"\n",
    "        Predicts n steps for all items in batch and return predictions\n",
    "        only for those steps (flattened)\n",
    "        steps: tensor of length B where each item is the number of steps that need to be taken\n",
    "        \"\"\"\n",
    "        n_users = batch[\"content_ids\"].shape[1]\n",
    "        lengths = batch[\"length\"]\n",
    "\n",
    "        users = torch.arange(n_users)\n",
    "\n",
    "        user_indexes = []\n",
    "        sequence_indexes = []\n",
    "\n",
    "        for i in range(steps.max().int(), 0, -1):\n",
    "            preds = model.process_batch_step(batch)\n",
    "\n",
    "            sequence_indexes_at_i = lengths[steps >= i] - i\n",
    "            user_indexes_at_i = users[steps >= i]\n",
    "\n",
    "            # could do sampling here instead of greedy\n",
    "            batch[\"answers\"][sequence_indexes_at_i, user_indexes_at_i] = (\n",
    "                preds[sequence_indexes_at_i, user_indexes_at_i] > 0.5\n",
    "            ).long()\n",
    "\n",
    "            user_indexes.append(user_indexes_at_i)\n",
    "            sequence_indexes.append(sequence_indexes_at_i)\n",
    "\n",
    "        if return_all_preds:\n",
    "            return preds\n",
    "\n",
    "        user_indexes = torch.cat(user_indexes)\n",
    "        sequence_indexes = torch.cat(sequence_indexes)\n",
    "\n",
    "        return (\n",
    "            preds[sequence_indexes, user_indexes],\n",
    "            batch[\"row_ids\"][sequence_indexes, user_indexes],\n",
    "        )\n",
    "\n",
    "    @auto_move_data\n",
    "    def predict_fast_single_user(\n",
    "        self, content_ids, parts, answers, tags, timestamps, n=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Predicts n steps for a single user in batch and return predictions\n",
    "        only for those steps (flattened)\n",
    "        \"\"\"\n",
    "        length = len(content_ids)\n",
    "        out_predictions = torch.zeros(n, device=self.device)\n",
    "        for i in range(n, 0, -1):\n",
    "            preds = self(content_ids, parts, answers, tags, timestamps)\n",
    "            out_predictions[n - i] = preds[length - i, 0]\n",
    "            answers[length - i] = (preds[length - i, 0] > 0.5).long()\n",
    "        return out_predictions\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        result = self.process_batch_step(batch)\n",
    "        loss = F.binary_cross_entropy(\n",
    "            result, batch[\"answered_correctly\"], weight=batch[\"loss_mask\"]\n",
    "        )\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_nb, dataset_nb=None):\n",
    "        # find a random number of steps to predict for\n",
    "        # number between 1 and 10 (randomly) - can never be x if min seq length is < x\n",
    "        random_step = torch.randint(\n",
    "            1, min(11, batch[\"length\"].min() + 1), batch[\"length\"].shape\n",
    "        )\n",
    "        result = self.predict_n_steps(batch, random_step, return_all_preds=True)\n",
    "        loss = F.binary_cross_entropy(\n",
    "            result, batch[\"answered_correctly\"], weight=batch[\"loss_mask\"]\n",
    "        )\n",
    "        self.log(\"val_loss_step\", loss)\n",
    "        select_mask = batch[\"loss_mask\"] > 0\n",
    "        positions = torch.cat(\n",
    "            result.shape[1] * [torch.arange(result.shape[0]).unsqueeze(1)], dim=1\n",
    "        )\n",
    "        return (\n",
    "            torch.masked_select(result, batch[\"loss_mask\"] > 0),\n",
    "            torch.masked_select(batch[\"answered_correctly\"], batch[\"loss_mask\"] > 0),\n",
    "            torch.masked_select(positions, select_mask),\n",
    "        )\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        y_pred = torch.cat([out[0] for out in outputs], dim=0)\n",
    "        y = torch.cat([out[1] for out in outputs], dim=0)\n",
    "        pos = torch.cat([out[2] for out in outputs], dim=0)\n",
    "        auc = auroc(y_pred, y)\n",
    "        self.log(\"avg_val_auc\", auc, prog_bar=True)\n",
    "\n",
    "        # Calculate accuracy per position\n",
    "        M = torch.zeros(pos.max() + 1, len(y), device=self.device)\n",
    "        M[pos, torch.arange(len(y))] = 1\n",
    "        M = torch.nn.functional.normalize(M, p=1, dim=1)\n",
    "        acc_per_position = torch.mm(\n",
    "            M, ((y_pred > 0.5) == y).float().unsqueeze(1)\n",
    "        ).flatten()\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        sns.regplot(\n",
    "            y=acc_per_position.cpu(), x=torch.arange(len(acc_per_position)).cpu(), ax=ax\n",
    "        )\n",
    "        ax.set_ylim(0.5, 1)\n",
    "        ax.set_xlim(0, len(acc_per_position) - 1)\n",
    "        ax.set_ylabel(\"acc\")\n",
    "        ax.set_xlabel(\"position\")\n",
    "        self.logger.experiment.add_figure(\n",
    "            \"val_acc_per_pos\", fig, global_step=self.current_epoch\n",
    "        )\n",
    "\n",
    "    def test_step(self, batch, batch_nb, dataset_nb=None):\n",
    "        # find a random number of steps to predict for\n",
    "        # number between 1 and 11 (randomly) - can never be x if min seq length is < x\n",
    "        random_step = torch.randint(\n",
    "            1, min(11, batch[\"length\"].min() + 1), batch[\"length\"].shape\n",
    "        )\n",
    "        result = self.predict_n_steps(batch, random_step, return_all_preds=True)\n",
    "\n",
    "        loss = F.binary_cross_entropy(\n",
    "            result, batch[\"answered_correctly\"], weight=batch[\"loss_mask\"]\n",
    "        )\n",
    "        self.log(\"test_loss_step\", loss)\n",
    "        select_mask = batch[\"loss_mask\"] > 0\n",
    "\n",
    "        # positions indexes (S, B)\n",
    "        positions = torch.cat(\n",
    "            result.shape[1] * [torch.arange(result.shape[0]).unsqueeze(1)], dim=1\n",
    "        )\n",
    "        return (\n",
    "            torch.masked_select(result, select_mask),\n",
    "            torch.masked_select(batch[\"answered_correctly\"], select_mask),\n",
    "            torch.masked_select(positions, select_mask),\n",
    "        )\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        y_pred = torch.cat([out[0] for out in outputs], dim=0)\n",
    "        y = torch.cat([out[1] for out in outputs], dim=0)\n",
    "        pos = torch.cat([out[2] for out in outputs], dim=0)\n",
    "        auc = auroc(y_pred, y)\n",
    "        self.log(\"avg_test_auc\", auc, prog_bar=True)\n",
    "\n",
    "        # Calculate accuracy per position\n",
    "        M = torch.zeros(pos.max() + 1, len(y), device=self.device)\n",
    "        M[pos, torch.arange(len(y))] = 1\n",
    "        M = torch.nn.functional.normalize(M, p=1, dim=1)\n",
    "        acc_per_position = torch.mm(\n",
    "            M, ((y_pred > 0.5) == y).float().unsqueeze(1)\n",
    "        ).flatten()\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        sns.regplot(\n",
    "            y=acc_per_position.cpu(), x=torch.arange(len(acc_per_position)).cpu(), ax=ax\n",
    "        )\n",
    "        ax.set_ylim(0.5, 1)\n",
    "        ax.set_xlim(0, len(acc_per_position) - 1)\n",
    "        ax.set_ylabel(\"acc\")\n",
    "        ax.set_xlabel(\"position\")\n",
    "        self.logger.experiment.add_figure(\"test_acc_per_pos\", fig, global_step=1)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode=\"max\", patience=10\n",
    "            ),\n",
    "            \"monitor\": \"avg_val_auc\",\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "            \"strict\": True,\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                     | Type               | Params\n",
      "----------------------------------------------------------------\n",
      "0 | embed_content_id         | Embedding          | 865 K \n",
      "1 | embed_parts              | Embedding          | 512   \n",
      "2 | embed_tags               | Embedding          | 12.1 K\n",
      "3 | embed_answered_correctly | Embedding          | 256   \n",
      "4 | embed_timestamps         | Linear             | 128   \n",
      "5 | pos_encoder              | PositionalEncoding | 0     \n",
      "6 | transformer              | Transformer        | 233 K \n",
      "7 | out_linear               | Linear             | 130   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5d455de2d74e1d9d825add7b02ecb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "\n",
    "# Params\n",
    "learning_rate = 0.001  # 0.0001\n",
    "emb_dim = 64  # 256\n",
    "dropout = 0.0\n",
    "n_heads = 2  # 2\n",
    "n_encoder_layers = 2\n",
    "n_decoder_layers = 2\n",
    "dim_feedforward = 256\n",
    "batch_size = 256\n",
    "num_user_train = 300000\n",
    "num_user_val = 30000\n",
    "max_window_size = WINDOW_SIZE\n",
    "\n",
    "\n",
    "# create split\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset,\n",
    "    [num_user_train, num_user_val, len(dataset) - num_user_train - num_user_val,],\n",
    ")\n",
    "\n",
    "# Init DataLoader from RIIID Dataset subset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=6,\n",
    "    pin_memory=torch.cuda.is_available(),  # if GPU then pin memory for perf\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=512,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=6,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "\n",
    "# Init our model\n",
    "model = RIIDDTransformerModel(\n",
    "    learning_rate=learning_rate,\n",
    "    emb_dim=emb_dim,  # embedding dimension - this is for everything\n",
    "    dropout=dropout,\n",
    "    n_heads=n_heads,\n",
    "    n_encoder_layers=n_encoder_layers,\n",
    "    n_decoder_layers=n_decoder_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    batch_size=batch_size,\n",
    "    num_user_train=num_user_train,\n",
    "    num_user_val=num_user_val,\n",
    "    max_window_size=max_window_size,\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    \"lightning_logs\",\n",
    "    name=\"submission_model_200\",\n",
    ")\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=500,\n",
    "    progress_bar_refresh_rate=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"avg_val_auc\", patience=20, mode=\"max\"),\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"avg_val_auc\",\n",
    "            filename=\"{epoch}-{val_loss_step:.2f}-{avg_val_auc:.2f}\",\n",
    "            mode=\"max\",\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# Train the model ⚡\n",
    "trainer.fit(model, train_dataloader=train_loader, val_dataloaders=[val_loader], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113e22c6c48e429184606239fb39dcb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'avg_test_auc': tensor(0.7813, device='cuda:0'),\n",
      " 'avg_val_auc': tensor(0.8107, device='cuda:0'),\n",
      " 'test_loss_step': tensor(0.3059, device='cuda:0'),\n",
      " 'train_loss': tensor(0.2776, device='cuda:0'),\n",
      " 'val_loss_step': tensor(0.2911, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'train_loss': 0.2776028513908386,\n",
       "  'avg_val_auc': 0.8106763362884521,\n",
       "  'val_loss_step': 0.2911358177661896,\n",
       "  'avg_test_auc': 0.7813144326210022,\n",
       "  'test_loss_step': 0.3058558404445648}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load best\n",
    "model = RIIDDTransformerModel.load_from_checkpoint(\n",
    "    \"lightning_logs/small_best_win_100/version_20/checkpoints/epoch=6-val_loss_step=0.31-avg_val_auc=0.78.ckpt\"\n",
    ")\n",
    "\n",
    "model.freeze()\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=512,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=6,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "trainer.test(\n",
    "    model=model, test_dataloaders=test_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Kaggle Sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceRIIDDataset(Dataset):\n",
    "    \"\"\"RIID dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, hdf5_file=\"feats.h5\", window_size=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_mapping (np.array): array of all unique user ids \n",
    "            hf5_file (string): location of hf5 feats file\n",
    "        \"\"\"\n",
    "        self.hdf5_file = hdf5_file\n",
    "        self.max_window_size = window_size\n",
    "        self.cache = {}\n",
    "\n",
    "    def open_hdf5(self):\n",
    "        # opens the h5py file\n",
    "        self.f = h5py.File(self.hdf5_file, \"r\")\n",
    "\n",
    "    def __len__(self):\n",
    "        if not hasattr(self, \"f\"):\n",
    "            self.open_hdf5()\n",
    "        return len(self.f.keys())\n",
    "\n",
    "    def load_user_into_cache(self, user_id):\n",
    "        \"\"\"\n",
    "        add a user to self.cache\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, \"f\"):\n",
    "            self.open_hdf5()\n",
    "        if f\"{user_id}\" in self.f:\n",
    "            content_ids = self.f[f\"{user_id}/content_ids\"]\n",
    "            parts = self.f[f\"{user_id}/parts\"]\n",
    "            answers = self.f[f\"{user_id}/answered_correctly\"]\n",
    "            timestamps = self.f[f\"{user_id}/timestamps\"]\n",
    "\n",
    "            self.cache[user_id] = {\n",
    "                \"content_ids\": content_ids,\n",
    "                \"parts\": parts,\n",
    "                \"answers\": answers,\n",
    "                \"timestamps\": timestamps,\n",
    "                \"row_ids\": np.zeros(len(timestamps)),\n",
    "                \"steps\": 0,\n",
    "            }\n",
    "        else:\n",
    "            self.cache[user_id] = {\n",
    "                \"content_ids\": np.array([], dtype=\"int64\"),\n",
    "                \"parts\": np.array([], dtype=\"int64\"),\n",
    "                \"timestamps\": np.array([], dtype=\"float32\"),\n",
    "                \"answers\": np.array([], dtype=\"int64\"),\n",
    "                \"row_ids\": np.array([], dtype=\"int64\"),\n",
    "                \"steps\": 0,\n",
    "            }\n",
    "\n",
    "    def update_user_rows(self, user_rows):\n",
    "        if not hasattr(self, \"f\"):\n",
    "            self.open_hdf5()\n",
    "\n",
    "        user_id = user_rows.user_id.values[0]\n",
    "        num_rows = len(user_rows)\n",
    "        new_content_ids = user_rows.content_id.values\n",
    "        new_parts = user_rows.part.values\n",
    "        new_timestamps = user_rows.timestamp.values\n",
    "        new_correctly_answered = np.ones(num_rows) * 3  # placeholder - padding with 3\n",
    "        new_row_ids = user_rows.row_id.values\n",
    "\n",
    "        if user_id not in self.cache:\n",
    "            self.load_user_into_cache(user_id)\n",
    "\n",
    "        self.cache[user_id][\"content_ids\"] = np.concatenate(\n",
    "            [self.cache[user_id][\"content_ids\"], new_content_ids]\n",
    "        )[-(self.max_window_size + 1) :]\n",
    "\n",
    "        self.cache[user_id][\"parts\"] = np.concatenate(\n",
    "            [self.cache[user_id][\"parts\"], new_parts]\n",
    "        )[-(self.max_window_size + 1) :]\n",
    "\n",
    "        self.cache[user_id][\"timestamps\"] = np.concatenate(\n",
    "            [self.cache[user_id][\"timestamps\"], new_timestamps]\n",
    "        )[-(self.max_window_size + 1) :]\n",
    "\n",
    "        self.cache[user_id][\"answers\"] = np.concatenate(\n",
    "            [self.cache[user_id][\"answers\"], new_correctly_answered]\n",
    "        )[-(self.max_window_size + 1) :]\n",
    "\n",
    "        self.cache[user_id][\"row_ids\"] = np.concatenate(\n",
    "            [self.cache[user_id][\"row_ids\"], new_row_ids]\n",
    "        )[-(self.max_window_size + 1) :]\n",
    "\n",
    "        self.cache[user_id][\"steps\"] = len(new_row_ids)\n",
    "\n",
    "    def update_answered_correctly(self, answered_correctly_rows):\n",
    "        user_id = answered_correctly_rows.name\n",
    "        num_rows = len(answered_correctly_rows)\n",
    "        new_correctly_answered = answered_correctly_rows.values\n",
    "        self.cache[user_id][\"answers\"] = np.concatenate(\n",
    "            [self.cache[user_id][\"answers\"], new_correctly_answered]\n",
    "        )\n",
    "        self.cache[user_id][\"steps\"] = 0\n",
    "\n",
    "    def __getitem__(self, user_id):\n",
    "        if user_id not in self.cache:\n",
    "            self.load_user_into_cache(user_id)\n",
    "\n",
    "        length = len(self.cache[user_id][\"content_ids\"])\n",
    "        window_size = min(self.max_window_size, length)\n",
    "\n",
    "        content_ids = self.cache[user_id][\"content_ids\"][-window_size:]\n",
    "        parts = self.cache[user_id][\"parts\"][-window_size:]\n",
    "        answers = self.cache[user_id][\"answers\"][-window_size:]\n",
    "        timestamps = self.cache[user_id][\"timestamps\"][-window_size:]\n",
    "        row_ids = self.cache[user_id][\"row_ids\"][-window_size:]\n",
    "\n",
    "        # convert timestamps to time elapsed\n",
    "        time_elapsed_timestamps = get_time_elapsed_from_timestamp(timestamps)\n",
    "\n",
    "        # get question tags\n",
    "        tags = questions_tags[content_ids, :].astype(np.int64)\n",
    "\n",
    "        # shift by one the answered_correctly sequence\n",
    "        answers = np.roll(answers, 1)\n",
    "\n",
    "        if length > self.max_window_size:\n",
    "            answers[0] = self.cache[user_id][\"answers\"][-window_size - 1]\n",
    "        else:\n",
    "            answers[0] = 2\n",
    "\n",
    "        return {\n",
    "            \"parts\": torch.from_numpy(parts).long(),\n",
    "            \"tags\": torch.from_numpy(tags).long(),\n",
    "            \"content_ids\": torch.from_numpy(content_ids).long(),\n",
    "            \"answers\": torch.from_numpy(answers).long(),\n",
    "            \"timestamps\": torch.from_numpy(time_elapsed_timestamps).float(),\n",
    "            \"length\": window_size,\n",
    "            \"row_ids\": torch.from_numpy(row_ids).int(),\n",
    "            \"steps\": self.cache[user_id][\"steps\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# The collate function is used to merge individual data samples into a batch\n",
    "# It handles the padding aspect\n",
    "def inference_collate_fn(batch):\n",
    "    # collate lenghts into 1D tensor\n",
    "    items = {\n",
    "        \"length\": torch.tensor([batch_item[\"length\"] for batch_item in batch]),\n",
    "        \"steps\": torch.tensor([batch_item[\"steps\"] for batch_item in batch]),\n",
    "    }\n",
    "\n",
    "    # find shape that the batch will have\n",
    "    max_length = items[\"length\"].max()\n",
    "    num_items = len(batch)\n",
    "    if num_items > 1:\n",
    "        # padding list\n",
    "        for (key, padding) in [\n",
    "            (\"parts\", 0),\n",
    "            (\"content_ids\", 13523),\n",
    "            (\"answers\", 3),\n",
    "            (\"timestamps\", 0.0),  # note timestamps isnt an embedding\n",
    "            (\"tags\", 188),\n",
    "            (\"row_ids\", 0),\n",
    "        ]:\n",
    "            items[key] = pad_sequence(\n",
    "                [batch_item[key] for batch_item in batch],\n",
    "                batch_first=False,\n",
    "                padding_value=padding,\n",
    "            )\n",
    "    else:\n",
    "        for key in [\"parts\", \"content_ids\", \"answers\", \"timestamps\", \"tags\", \"row_ids\"]:\n",
    "            items[key] = batch[0][key].unsqueeze(1)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "model.freeze()\n",
    "model.cuda()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_df(df):\n",
    "    unique_users = df.user_id.unique()\n",
    "    if len(unique_users) == 1:\n",
    "        item = inference_dataset[unique_users[0]].copy()\n",
    "        predictions = model.predict_fast_single_user(\n",
    "            item[\"content_ids\"],\n",
    "            item[\"parts\"],\n",
    "            item[\"answers\"],\n",
    "            item[\"tags\"],\n",
    "            item[\"timestamps\"],\n",
    "            n=item[\"steps\"],\n",
    "        ).cpu()\n",
    "        df[\"answered_correctly\"] = predictions\n",
    "    else:\n",
    "        batch = inference_collate_fn([inference_dataset[u].copy() for u in unique_users])\n",
    "        predictions, row_ids = model.predict_n_steps(batch, batch[\"steps\"])\n",
    "        df[\"answered_correctly\"] = df[\"row_id\"].map(\n",
    "            dict(zip(row_ids.cpu().numpy(), predictions.cpu().numpy()))\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5485018"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_groups = np.array_split(valid, len(valid) // 18) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = InferenceRIIDDataset(hdf5_file=\"feats_train.h5\",)\n",
    "previous_test_df = None\n",
    "predictions = []\n",
    "row_ids = []\n",
    "for current_test in tqdm(valid_groups):\n",
    "    if previous_test_df is not None:\n",
    "        previous_test_df[previous_test_df.content_type_id == 0].groupby(\n",
    "            \"user_id\"\n",
    "        ).answered_correctly.apply(inference_dataset.update_answered_correctly)\n",
    "\n",
    "    current_test = current_test.copy()\n",
    "    \n",
    "    # your feature extraction and model training code here\n",
    "    previous_test_df = current_test.copy()\n",
    "\n",
    "    # add current to cache\n",
    "    current_test = current_test[current_test.content_type_id == 0]\n",
    "\n",
    "    if len(current_test) < 1:\n",
    "        continue\n",
    "\n",
    "    current_test = add_part_to_questions(current_test)\n",
    "    current_test[[\"row_id\", \"user_id\", \"content_id\", \"part\", \"timestamp\"]].groupby(\"user_id\").apply(\n",
    "        lambda user_rows: inference_dataset.update_user_rows(user_rows)\n",
    "    )\n",
    "    \n",
    "\n",
    "    # your prediction code here\n",
    "    current_test = predict_for_df(current_test)\n",
    "    predictions.append(current_test[\"answered_correctly\"].values)\n",
    "    row_ids.append(current_test[\"row_id\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = np.concatenate(predictions)\n",
    "all_row_ids = np.concatenate(row_ids)\n",
    "df = pd.DataFrame({\"predictions\": all_preds, \"row_id\": all_row_ids})\n",
    "df = df.merge(valid[[\"row_id\", \"user_id\", \"answered_correctly\"]], on=\"row_id\", how=\"left\")\n",
    "print('validation auc:',roc_auc_score(df.answered_correctly.values, df.predictions.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def roc_score_per_interaction(x):\n",
    "    try:\n",
    "        return roc_auc_score(x.answered_correctly.values, x.predictions.values)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df[\"num_iteractions\"] = (df.groupby(\"user_id\").cumcount() // 10) * 10\n",
    "interactions_auc = df.groupby(\"num_iteractions\").apply(roc_score_per_interaction)\n",
    "interactions_auc = interactions_auc[~interactions_auc.isna()].copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.regplot(\n",
    "    y=interactions_auc.values,\n",
    "    x=interactions_auc.index,\n",
    "    line_kws={\"color\": \"red\", \"linewidth\": 2},\n",
    "    scatter_kws={\"s\": 3},\n",
    ")\n",
    "ax.set_ylim(0.5, 1)\n",
    "ax.set_xlim(0, interactions_auc.index.max())\n",
    "ax.set_ylabel(\"auc\")\n",
    "ax.set_xlabel(\"number of seen interactions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation score\n",
    "y_pred = pd.concat(predicted).answered_correctly\n",
    "y_true = valid[valid.content_type_id == 0].answered_correctly[:len(y_pred)]\n",
    "\n",
    "print('validation auc:',roc_auc_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Iter_Valid(object):\n",
    "    def __init__(self, df, max_user=1000):\n",
    "        df = df.reset_index(drop=True)\n",
    "        self.df = df\n",
    "        self.user_answer = df[\"user_answer\"].astype(str).values\n",
    "        self.answered_correctly = df[\"answered_correctly\"].astype(str).values\n",
    "        df[\"prior_group_responses\"] = \"[]\"\n",
    "        df[\"prior_group_answers_correct\"] = \"[]\"\n",
    "        self.sample_df = df[df[\"content_type_id\"] == 0][[\"row_id\"]]\n",
    "        self.sample_df[\"answered_correctly\"] = 0\n",
    "        self.len = len(df)\n",
    "        self.user_id = df.user_id.values\n",
    "        self.task_container_id = df.task_container_id.values\n",
    "        self.content_type_id = df.content_type_id.values\n",
    "        self.max_user = max_user\n",
    "        self.current = 0\n",
    "        self.pre_user_answer_list = []\n",
    "        self.pre_answered_correctly_list = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def fix_df(self, user_answer_list, answered_correctly_list, pre_start):\n",
    "        df = self.df[pre_start : self.current].copy()\n",
    "        sample_df = self.sample_df[pre_start : self.current].copy()\n",
    "        df.loc[pre_start, \"prior_group_responses\"] = (\n",
    "            \"[\" + \",\".join(self.pre_user_answer_list) + \"]\"\n",
    "        )\n",
    "        df.loc[pre_start, \"prior_group_answers_correct\"] = (\n",
    "            \"[\" + \",\".join(self.pre_answered_correctly_list) + \"]\"\n",
    "        )\n",
    "        self.pre_user_answer_list = user_answer_list\n",
    "        self.pre_answered_correctly_list = answered_correctly_list\n",
    "        return df, sample_df\n",
    "\n",
    "    def __next__(self):\n",
    "        added_user = set()\n",
    "        pre_start = self.current\n",
    "        pre_added_user = -1\n",
    "        pre_task_container_id = -1\n",
    "\n",
    "        user_answer_list = []\n",
    "        answered_correctly_list = []\n",
    "        while self.current < self.len:\n",
    "            crr_user_id = self.user_id[self.current]\n",
    "            crr_task_container_id = self.task_container_id[self.current]\n",
    "            crr_content_type_id = self.content_type_id[self.current]\n",
    "            if crr_content_type_id == 1:\n",
    "                # no more than one task_container_id of \"questions\" from any single user\n",
    "                # so we only care for content_type_id == 0 to break loop\n",
    "                user_answer_list.append(self.user_answer[self.current])\n",
    "                answered_correctly_list.append(self.answered_correctly[self.current])\n",
    "                self.current += 1\n",
    "                continue\n",
    "            if crr_user_id in added_user and (\n",
    "                (crr_user_id != pre_added_user)\n",
    "                or (crr_task_container_id != pre_task_container_id)\n",
    "            ):\n",
    "                # known user(not prev user or differnt task container)\n",
    "                return self.fix_df(user_answer_list, answered_correctly_list, pre_start)\n",
    "            if len(added_user) == self.max_user:\n",
    "                if (\n",
    "                    crr_user_id == pre_added_user\n",
    "                    and crr_task_container_id == pre_task_container_id\n",
    "                ):\n",
    "                    user_answer_list.append(self.user_answer[self.current])\n",
    "                    answered_correctly_list.append(\n",
    "                        self.answered_correctly[self.current]\n",
    "                    )\n",
    "                    self.current += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    return self.fix_df(\n",
    "                        user_answer_list, answered_correctly_list, pre_start\n",
    "                    )\n",
    "            added_user.add(crr_user_id)\n",
    "            pre_added_user = crr_user_id\n",
    "            pre_task_container_id = crr_task_container_id\n",
    "            user_answer_list.append(self.user_answer[self.current])\n",
    "            answered_correctly_list.append(self.answered_correctly[self.current])\n",
    "            self.current += 1\n",
    "        if pre_start < self.current:\n",
    "            return self.fix_df(user_answer_list, answered_correctly_list, pre_start)\n",
    "        else:\n",
    "            raise StopIteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inference_dataset = InferenceRIIDDataset(hdf5_file=\"feats_train.h5\",)\n",
    "iter_test = Iter_Valid(valid,max_user=1000)\n",
    "predicted = []\n",
    "def set_predict(df):\n",
    "    predicted.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac18994dbb7434e8e8694d8580451a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2500000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbar = tqdm(total=len(valid))\n",
    "previous_test_df = None\n",
    "for (current_test, current_prediction_df) in iter_test:\n",
    "    if previous_test_df is not None:\n",
    "        previous_test_df[\"answered_correctly\"] = eval(\n",
    "            current_test[\"prior_group_answers_correct\"].iloc[0]\n",
    "        )\n",
    "        previous_test_df[previous_test_df.content_type_id == 0].groupby(\n",
    "            \"user_id\"\n",
    "        ).answered_correctly.apply(inference_dataset.update_answered_correctly)\n",
    "\n",
    "    # your feature extraction and model training code here\n",
    "    previous_test_df = current_test.copy()\n",
    "\n",
    "    # add current to cache\n",
    "    current_test = current_test[current_test.content_type_id == 0]\n",
    "    current_test = add_part_to_questions(current_test)\n",
    "    current_test[[\"row_id\", \"user_id\", \"content_id\", \"part\", \"timestamp\"]].groupby(\n",
    "        \"user_id\"\n",
    "    ).apply(lambda user_rows: inference_dataset.update_user_rows(user_rows))\n",
    "\n",
    "    # your prediction code here\n",
    "    current_test = predict_for_df(current_test)\n",
    "    set_predict(current_test.loc[:, [\"row_id\", \"answered_correctly\"]])\n",
    "    pbar.update(len(current_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation auc: 0.7301190955694308\n"
     ]
    }
   ],
   "source": [
    "#validation score\n",
    "y_pred = pd.concat(predicted).answered_correctly\n",
    "y_true = valid[valid.content_type_id == 0].answered_correctly[:len(y_pred)]\n",
    "\n",
    "print('validation auc:',roc_auc_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
