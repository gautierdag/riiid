{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.style as style\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "style.use(\"fivethirtyeight\")\n",
    "import gc\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "from pytorch_lightning import seed_everything\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "\n",
    "SEED = 69\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lectures arrays\n",
      "Loading questions arrays\n",
      "CPU times: user 32.7 ms, sys: 2.72 s, total: 2.75 s\n",
      "Wall time: 5.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This code is needed for dataset\n",
    "train = pd.read_pickle(\"riiid_train.pkl.gzip\")\n",
    "questions_df = pd.read_csv(\"data/questions.csv\")\n",
    "lectures_df = pd.read_csv(\"data/lectures.csv\")\n",
    "\n",
    "folder_path = \"data\"\n",
    "print(\"Loading lectures arrays\")\n",
    "lectures_ids = np.load(f\"{folder_path}/lectures_ids.npy\")\n",
    "lectures_parts = np.load(f\"{folder_path}/lectures_parts.npy\")\n",
    "lectures_types = np.load(f\"{folder_path}/lectures_types.npy\")\n",
    "lectures_tags = lectures_df.tag.values\n",
    "\n",
    "print(\"Loading questions arrays\")\n",
    "questions_parts = np.load(f\"{folder_path}/questions_parts.npy\")\n",
    "questions_lectures_parts = np.concatenate([questions_parts, lectures_parts])\n",
    "\n",
    "\n",
    "# process tags\n",
    "def split_tags(t):\n",
    "    try:\n",
    "        return [int(i) for i in t.split(\" \")]\n",
    "    except AttributeError:\n",
    "        return list()\n",
    "\n",
    "\n",
    "# Get tags to be 2D array of shape (Q, T), where Q is question_idx, and T is the max number of tag possible (6)\n",
    "questions_df[\"tags\"] = questions_df.tags.apply(split_tags)\n",
    "questions_tags = pd.DataFrame(questions_df[\"tags\"].tolist(), index=questions_df.index)\n",
    "\n",
    "# map lecture id to new id\n",
    "\n",
    "lectures_mapping = dict(zip(lectures_df.lecture_id.values,(lectures_df.index + 13523).values))\n",
    "lectures_df.lecture_id = lectures_df.index + 13523\n",
    "lectures_tags = pd.DataFrame(lectures_df.tag.values, index=lectures_df.lecture_id.values)\n",
    "\n",
    "questions_lectures_tags = pd.concat([questions_tags, lectures_tags])\n",
    "# pad with max tag + 1\n",
    "questions_lectures_tags = (\n",
    "    questions_lectures_tags.fillna(questions_lectures_tags.max().max() + 1)\n",
    "    .astype(np.int)\n",
    "    .values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    \"\"\"\n",
    "    Converts the lecture ids to proper content_ids\n",
    "    Adds the answered_correctly column if not exists\n",
    "    \"\"\"\n",
    "    df.content_type_id = df.content_type_id.astype(bool)\n",
    "    \n",
    "    # prior information\n",
    "    df.prior_question_had_explanation = df.prior_question_had_explanation.fillna(0).astype(np.uint8)\n",
    "    df.prior_question_elapsed_time = df.prior_question_elapsed_time.fillna(0).clip(upper=300000)/300000 #normalizes to 0-1\n",
    "    \n",
    "    # map lecture ids to new content_ids\n",
    "    df.loc[df.content_type_id, \"content_id\"] = df[df.content_type_id].content_id.map(\n",
    "        lectures_mapping\n",
    "    )\n",
    "    # if not answered correctly then add column with\n",
    "    # y = 3 (padding) for all questions and y = 4 for lectures\n",
    "    if \"answered_correctly\" not in df.columns:\n",
    "        df[\"answered_correctly\"] = df.content_type_id.map({False: 3, True: 4})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0000001\n",
    "def get_time_elapsed_from_timestamp(arr):\n",
    "    arr_seconds = np.diff(arr, prepend=0) / 1000\n",
    "    return (np.log(arr_seconds + eps).astype(np.float32)-3.5) / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.3 s, sys: 1.55 s, total: 41.8 s\n",
      "Wall time: 41.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEJCAYAAABsc6siAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAASA0lEQVR4nO3de1BU9f/H8ZeBipcKRxEzb4XrJcNIFEYdL+FlLA3RUnS6TFqmThen8oY1OpOWmo5OTDqaSl7SlJS4JKNfZ3ISzVumTmkRWWpqoeElTVBBfn/4Y8cVkH3DLuvl+ZhxRs4ez37ee2CfnN1tqnL27NlCAQBgcI+vFwAAuP0QDwCAGfEAAJgRDwCAGfEAAJgRDwCAGfEAAJgRDwCAmU/ikZWV5Yu79ag7YQaJOW4ld8IMEnPcarw1B1ceAAAz4gEAMCMeAAAz4gEAMCMeAAAz4gEAMCMeAAAz4gEAMCMeAAAz4gEAMCMeAAAz4gEAMCMeAAAz4gEAMCMeAAAz4gEAMCMeAAAz4gEAMPP39QKAO1lMTIxXjpucnOyV4wLu4soDAGBGPAAAZsQDAGDGex5AJcrqPanE7Y7/fXjT/W68HfA1rjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBgRjwAAGbEAwBg5u/rBQC+EBMT4/J1cnKyT9Zxq+Fxgbu48gAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAIAZ8QAAmBEPAICZvy/vPCYmxvn35ORkn63jbhEbG6tLly4pICBAq1evdrlt6NChys3NVa1atbRy5UoNHDhQV69elZ+fn6pVq6bc3FwfrRq+dP3PKG4v48aNU+fOnb12fK487iKXLl2SJOXl5RW7rSgO//33nyTp6tWrkqSCggLCAdyG5s6d69Xj++zK48bfaGJiYrj68KLY2FiXr4cMGeK8+hg6dKjLbXfjb5u328y323pR+fLz87Vt2zbVr1/fK8fnyuMuUXTVUeT6qw+uLIA7kzevPogHANyh8vPzvXZs4gEAdyh/f++9M+HTT1uh8lSvXt3lpauAgADn32vUqMFLV/LOJ/689d6Et94f5L2UO8tbb73ltWP77Mrjxm9+3iz3rjVr1rh8ff1Hdb/44guX2+7Gc3E3zow7m7+/Px/VhWdUr15dkutVR5EaNWpIkmrVqiVJuueea98afn5+ztsA3D68edUh+fhlK37bq1w3Xn1c78arj6SkJG8vx6OysrLkcDjc3p+XZ9xTnp9R67m4Vd0pc3gLVx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAw8/f1AgBfSE5O9vUSbkk8LnAXVx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAwIx4AADPiAQAw8/f1AoC7ieN/H3p0P8BXuPIAAJgRDwCAGfEAAJjxngfgRcnJyW7vm5WVJYfD4b3FAB7ElQcAwIx4AADMiAcAwIx4AADMiAcAwIx4AADMiAcAwIx4AADMiAcAwIx4AADMiAcAwIx4AADMiAcAwIx4AADMiAcAwIx4AADMiAcAwIx4AADMiAcAwKzK2bNnC329CADA7YUrDwCAGfEAAJgRDwCAGfEAAJgRDwCAmcfjsXTpUvXr109NmjRRYGCgjhw54ta/S0lJUWRkpOrXr6/IyEilpaW53F5YWKjp06erVatWatCggfr27auff/7Z08uXJF26dEnjxo3Tww8/rIYNG2rIkCE6fvz4Tf9NaGioAgMDi/0ZPHiwc5/p06cXu71FixZemaG8c7izxso8F+WdY86cOXriiSfUuHFjhYSEKDY2VgcPHnTZZ/To0cVm7dmzp8fWvXjxYrVt21bBwcHq1q2bvvvuu5vuv3XrVnXr1k3BwcF67LHHlJCQUOFjVpTl/lJTUzVgwACFhISoUaNG6tGjh9LT0132WblyZYk/J3l5ebfMHBkZGSWu8ddff3XZr6znLG+wzFHS93dgYKAaNmzo3MfdWUvi8XhcvHhRUVFRmjhxotv/ZteuXRo+fLgGDRqkjIwMDRo0SC+99JK+//575z4ff/yx5s2bp5kzZ+qbb75RUFCQBgwYoPPnz3t6BMXFxSktLU1LlixRenq6zp8/r9jYWBUUFJT6bzZv3qzMzEznn2+//VZVqlRRTEyMy34Oh8NlP2/+8JdnDnfWWJnnorxzbN26VS+//LI2btyo1NRU+fv7KyYmRmfOnHHZr3v37i6zfvnllx5Zc1JSkiZOnKh33nlHW7ZsUUREhAYNGqQ///yzxP0PHz6swYMHKyIiQlu2bNHbb7+t8ePHKyUlpdzHrOwZtm3bpq5duyoxMVFbtmxRr1699Pzzzxf7/qlZs6bLY56ZmamAgACvzFCeOYrs2LHDZY0hISHO29x5zvL1HDNmzCj2ODdr1qzYc5J081lL47X/zmPv3r164okntH//fjVt2vSm+w4bNkxnzpxRcnKyc1v//v1Vr149LVmyRIWFhWrVqpVGjBihsWPHSpJyc3PlcDg0depUDRs2zGPrPnfunJo3b6558+Y5rxqOHTum0NBQrV27Vj169HDrOLNnz1Z8fLwyMzNVo0YNSdd+q09NTdX27ds9tt7SlHeOstZYmeeiInPc6MKFC2rSpIlWrlypJ598UtK138xOnz6tNWvWeHTNktSjRw+1adNG8fHxzm3t2rVT//79NWXKlGL7T5kyRWlpafrhhx+c29544w398ssv2rRpU7mOWdkzlCQqKkodO3bUBx98IOnalcf48ePLvHL0JOscGRkZevrpp3Xo0CHVrVu3xGOW9ZzlDRU9Hzt27FCfPn20ceNGRUZGSnJv1tLcEu957N69W1FRUS7bevTooZ07d0qSjhw5ouzsbJd9atSooU6dOjn38ZR9+/bpypUrLvfVqFEjtWzZ0u37Kiws1IoVKxQbG+sMR5HDhw+rVatWatu2rYYPH67Dhw97cvlOFZnjZmuszHNR0Tmud+HCBV29elWBgYEu27dv367mzZsrPDxcb775pk6dOlXhNV++fFn79u0r9j0dFRVV6pp37dpV4s/A3r17deXKlXIdsyI8dX8XLlwo9pjn5ubq0Ucf1SOPPKLY2Fjt37/fE0suUUXm6N69u1q2bKno6Ght2bLF5baynrM8zRPnY9myZWrdurUzHNe72ayluSXikZ2draCgIJdtQUFBOnnypPP2om2l7eMpJ0+elJ+fX7EKW+5r8+bNOnLkiF588UWX7e3bt9f8+fO1du1axcfHKzs7W71799bp06c9tv4i5Z2jrDVW5rmQPHM+JGnixIkKDQ1VRESEc1vPnj21YMECpaSkaNq0adqzZ4+io6N16dKlCq05JydHBQUFpsfo5MmTJe6fn5+vnJycch2zIjxxf4sWLdKJEycUGxvr3OZwOPTJJ59o1apVWrx4sapXr64+ffro0KFDHl1/kfLM0aBBA82ZM0crVqzQihUr5HA41L9/f5eX38p6zvK0ip6Pc+fOKTk5udhzkjuzlsbfnYVPmzZNs2fPvuk+aWlp6tKlizuH8wl3Z/CEZcuWqV27dgoNDXXZ3qtXL5ev27dvr7CwMK1atUqvv/66W8f29hyeWKM7KvN8TJo0STt27NCGDRvk5+fn3P7MM884/96mTRuFhYUpNDRUGzduVHR0tEfu+26VkpKiyZMnKyEhQU2aNHFuj4iIcAl4ZGSkunTpooULF+qjjz7yxVKLcTgccjgczq8jIiJ09OhRxcfHq1OnTj5cWfklJibq6tWrGjJkiMv2iszqVjxGjx7t8qmhkjRq1MidQ5UoODi42MsFp06dUv369Z23F21r3LhxifuUxd0Zdu/erYKCAuXk5KhevXou99WxY8cy7+fUqVNKT08v84lRkmrXrq1WrVrp999/L3uA/1dZc5S2Rk+ci8qcIy4uTklJSUpLS1OzZs1uuu8DDzyghg0bms5HSerWrSs/P7+bfk/fqH79+iXu7+/vr7p166qwsNB8zIoozwxFUlJSNGrUKC1YsMD5/lJp/Pz8FBYWVuHHvDQVmeN64eHhSkpKcn5d1nOWp1V0jmXLlik6Olp16tQpc98bZy2NWy9b1a1bVy1atLjpn5o1a7pzqBJ16NBBmzdvdtm2efNm52tzTZs2VXBwsMs+eXl52r59e4mv31VkhrCwMFWtWtXlvo4fP67MzEy37mvVqlWqXr26y2+1pcnLy1NWVpbzCflWmqO0NXriXFTWHBMmTNC6deuUmprq1keic3Jy9Ndff5nOR0mqVaumsLCwm35P3ygiIqLE/R9//HFVrVq1XMesiPLe31dffaWRI0dq/vz56t+/f5n3U1hYqAMHDlT4MS+Npx63H3/80WWNZT1neVpF5tizZ49++umnYi9ZlebGWUvj1pWHRXZ2trKzs/Xbb79JkjIzM3Xu3Dk1btzYWb3o6GiFh4c7PyEwatQoPfXUU5o7d6769u2rr7/+WhkZGdqwYYMkqUqVKho9erTmzJkjh8Oh5s2ba/bs2apVq5aeffZZj67//vvv1wsvvKApU6YoKChIderU0bvvvqs2bdqoe/fuzv06dOigESNG6NVXX3VuKyws1PLlyzVw4EDVrl272LHfe+899enTR40aNdI///yjWbNm6eLFixo6dKhHZ6jIHGWtsTLPRUXmGDt2rNasWaPPP/9cgYGBzvdqatWqpdq1a+vChQuaMWOGoqOjFRwcrKNHj+r9999XUFCQ+vXrV+F1v/baaxo5cqTCw8MVGRmphIQE/f33385Po40cOVKStHDhQknXPr2zaNEiTZw4UcOGDdPOnTud7wu4e0xPs86wbt06jRw5UlOnTlWnTp2cj3m1atWcP/szZsxQhw4dFBISon///VcLFy7UgQMHNGfOHK/MUJ455s+fryZNmqh169a6fPmyEhMTtX79ei1fvtx5zLKes26FOYosXbpUISEhJb6t4M6spfF4PBISEjRz5kzn10UvTcybN0/PPfecJOmPP/7Qgw8+6Nyn6IGYNm2aPvzwQz300ENKSEhQ+/btnfuMGTNGubm5GjdunM6ePeu8tLr33ns9PYKmT58uPz8/DRs2THl5eeratasWLFjg8np5VlaWcnJyXP5dRkaGDh06pE8//bTE4544cUKvvPKK8yWY9u3ba9OmTS6vCft6DnfWWJnnorxzFD3p3vjb74QJExQXFyc/Pz8dPHhQq1ev1rlz5xQcHKwuXbros88+88gcAwcO1OnTpzVr1ixlZ2erdevWSkxMdD6Ox44dc9m/WbNmSkxM1KRJk5SQkKAGDRpo5syZLusv65ieZp0hISFB+fn5iouLU1xcnHN7586dtX79eknX3rgdM2aMTp48qfvuu09t27ZVenq6wsPDvTJDeea4cuWKJk+erBMnTiggIMC5f+/evZ37uPOc5es5JOn8+fNKSkrS+PHjSzymO7OWhv+fBwDA7Jb4qC4A4PZCPAAAZsQDAGBGPAAAZsQDAGBGPAAAZsQDAGBGPAAAZsQDAGD2f71y44oNCuoaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "## Show effects of transformation \n",
    "times = train.groupby(\"user_id\").timestamp.apply(get_time_elapsed_from_timestamp)\n",
    "times = np.concatenate(times.tolist())\n",
    "sns.boxplot(x=times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Hdf5\n",
    "\n",
    "Only run this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.21 s, sys: 328 ms, total: 1.54 s\n",
      "Wall time: 1.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = preprocess_df(train) #convert lecture ids to be sequentially following the question_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['row_id', 'timestamp', 'user_id', 'content_id', 'content_type_id',\n",
       "       'task_container_id', 'user_answer', 'answered_correctly',\n",
       "       'prior_question_elapsed_time', 'prior_question_had_explanation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.answered_correctly.replace(-1, 4, inplace=True) # set lecture to token 4 for answered correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d744f15ac0f45f2ad4bcbc4a6524d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=338170.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ignore lectures for now\n",
    "hf = h5py.File(\"train_feats.h5\", \"w\")\n",
    "\n",
    "for user_id, data in tqdm(train.groupby(\"user_id\")):\n",
    "    processed_feats = data[\n",
    "        [\n",
    "            \"content_id\",\n",
    "            \"answered_correctly\",\n",
    "            \"timestamp\",\n",
    "            \"prior_question_elapsed_time\",\n",
    "            \"prior_question_had_explanation\"\n",
    "        ]\n",
    "    ].values\n",
    "\n",
    "    hf.create_dataset(f\"{user_id}/content_ids\", data=processed_feats[:, 0], maxshape=(None,))\n",
    "    hf.create_dataset(f\"{user_id}/answered_correctly\", data=processed_feats[:, 1], maxshape=(None,))\n",
    "    hf.create_dataset(f\"{user_id}/timestamps\", data=processed_feats[:, 2], maxshape=(None,))\n",
    "    hf.create_dataset(f\"{user_id}/prior_question_elapsed_time\", data=processed_feats[:, 3], maxshape=(None,))\n",
    "    hf.create_dataset(f\"{user_id}/prior_question_had_explanation\", data=processed_feats[:, 4], maxshape=(None,))\n",
    "\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Here we define the pytorch Dataset object and a custom collate function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RIIDDataset(Dataset):\n",
    "    \"\"\"RIID dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_mapping,\n",
    "        user_history,\n",
    "        hdf5_file=\"feats.h5\",\n",
    "        window_size=100,\n",
    "        use_cache=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_mapping (np.array): array of user_ids per row\n",
    "            user_history (np.array): array of length of history up till this row\n",
    "            hdf5_file (string): location of hf5 feats file\n",
    "            window_size (int): size of window to lookback to max\n",
    "            use_cache (opt, bool): whether to cache reads\n",
    "        \"\"\"\n",
    "        # np array where index maps to a user id\n",
    "        self.user_mapping = user_mapping\n",
    "        self.user_history = user_history\n",
    "        self.hdf5_file = f\"{hdf5_file}\"\n",
    "        self.max_window_size = window_size\n",
    "        self.use_cache = use_cache\n",
    "        self.cache = {}\n",
    "\n",
    "    def open_hdf5(self):\n",
    "        # opens the h5py file\n",
    "        self.f = h5py.File(self.hdf5_file, \"r\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_mapping)\n",
    "\n",
    "    def load_user_into_cache(self, user_id):\n",
    "        \"\"\"\n",
    "        add a user to self.cache\n",
    "        \"\"\"\n",
    "\n",
    "        self.cache[user_id] = {\n",
    "            \"content_ids\": np.array(self.f[f\"{user_id}/content_ids\"], dtype=np.int64),\n",
    "            \"answered_correctly\": np.array(\n",
    "                self.f[f\"{user_id}/answered_correctly\"], dtype=np.int64\n",
    "            ),\n",
    "            \"timestamps\": np.array(self.f[f\"{user_id}/timestamps\"], dtype=np.float32),\n",
    "            \"prior_question_elapsed_time\": np.array(\n",
    "                self.f[f\"{user_id}/prior_question_elapsed_time\"], dtype=np.float32\n",
    "            ),\n",
    "            \"prior_question_had_explanation\": np.array(\n",
    "                self.f[f\"{user_id}/prior_question_had_explanation\"], dtype=np.int64\n",
    "            ),\n",
    "        }\n",
    "    def preload_all(self):\n",
    "        if not hasattr(self, \"f\"):\n",
    "            self.open_hdf5()\n",
    "        for u in tqdm(np.unique(dataset.user_mapping)):\n",
    "            self.load_user_into_cache(u)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # open the hdf5 file in the iterator to allow multiple workers\n",
    "        # https://github.com/pytorch/pytorch/issues/11929\n",
    "        if not hasattr(self, \"f\"):\n",
    "            self.open_hdf5()\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        user_id = self.user_mapping[idx]\n",
    "        length = self.user_history[idx]\n",
    "        # length = self.f[f\"{user_id}/answered_correctly\"].len()\n",
    "\n",
    "        window_size = min(self.max_window_size, length)\n",
    "\n",
    "        # index for loading larger than window size\n",
    "        start_index = 0\n",
    "        if length > window_size:\n",
    "            # randomly select window size subset instead of trying to cram in everything\n",
    "            start_index = length - window_size\n",
    "\n",
    "        if not self.use_cache:\n",
    "            content_ids = np.zeros(window_size, dtype=np.int64).copy()\n",
    "            answered_correctly = np.zeros(window_size, dtype=np.int64).copy()\n",
    "            timestamps = np.zeros(window_size, dtype=np.float32).copy()\n",
    "            prior_q_times = np.zeros(window_size, dtype=np.float32).copy()\n",
    "            prior_q_explanation = np.zeros(window_size, dtype=np.float32).copy()\n",
    "\n",
    "            self.f[f\"{user_id}/content_ids\"].read_direct(\n",
    "                content_ids,\n",
    "                source_sel=np.s_[start_index : start_index + window_size],\n",
    "                dest_sel=np.s_[0:window_size],\n",
    "            )\n",
    "            self.f[f\"{user_id}/answered_correctly\"].read_direct(\n",
    "                answered_correctly,\n",
    "                source_sel=np.s_[start_index : start_index + window_size],\n",
    "                dest_sel=np.s_[0:window_size],\n",
    "            )\n",
    "            self.f[f\"{user_id}/timestamps\"].read_direct(\n",
    "                timestamps,\n",
    "                source_sel=np.s_[start_index : start_index + window_size],\n",
    "                dest_sel=np.s_[0:window_size],\n",
    "            )\n",
    "            self.f[f\"{user_id}/prior_question_elapsed_time\"].read_direct(\n",
    "                prior_q_times,\n",
    "                source_sel=np.s_[start_index : start_index + window_size],\n",
    "                dest_sel=np.s_[0:window_size],\n",
    "            )\n",
    "            self.f[f\"{user_id}/prior_question_had_explanation\"].read_direct(\n",
    "                prior_q_explanation,\n",
    "                source_sel=np.s_[start_index : start_index + window_size],\n",
    "                dest_sel=np.s_[0:window_size],\n",
    "            )\n",
    "        else:\n",
    "            if user_id not in self.cache:\n",
    "                self.load_user_into_cache(user_id)\n",
    "\n",
    "            content_ids = self.cache[user_id][\"content_ids\"][\n",
    "                start_index : start_index + window_size\n",
    "            ].copy()\n",
    "            answered_correctly = self.cache[user_id][\"answered_correctly\"][\n",
    "                start_index : start_index + window_size\n",
    "            ].copy()\n",
    "            timestamps = self.cache[user_id][\"timestamps\"][\n",
    "                start_index : start_index + window_size\n",
    "            ].copy()\n",
    "            prior_q_times = self.cache[user_id][\"prior_question_elapsed_time\"][\n",
    "                start_index : start_index + window_size\n",
    "            ].copy()\n",
    "            prior_q_explanation = self.cache[user_id][\"prior_question_had_explanation\"][\n",
    "                start_index : start_index + window_size\n",
    "            ].copy()\n",
    "\n",
    "        # convert timestamps to time elapsed\n",
    "        time_elapsed_timestamps = get_time_elapsed_from_timestamp(timestamps)\n",
    "\n",
    "        # get question tags\n",
    "        tags = questions_lectures_tags[content_ids, :]\n",
    "\n",
    "        # get question parts\n",
    "        parts = questions_lectures_parts[content_ids]\n",
    "\n",
    "        # shift by one the answered_correctly sequence\n",
    "        answers = np.roll(answered_correctly, 1)\n",
    "\n",
    "        # set start token if start_index is actually first element\n",
    "        if start_index == 0:\n",
    "            answers[0] = 2\n",
    "        # else replace first element of sequence with actual previous element\n",
    "        else:\n",
    "            self.f[f\"{user_id}/answered_correctly\"].read_direct(\n",
    "                answers, source_sel=np.s_[start_index - 1], dest_sel=np.s_[0],\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"parts\": torch.from_numpy(parts).long(),\n",
    "            \"tags\": torch.from_numpy(tags).long(),\n",
    "            \"content_ids\": torch.from_numpy(content_ids),\n",
    "            \"answered_correctly\": torch.from_numpy(answered_correctly),\n",
    "            \"answers\": torch.from_numpy(answers),\n",
    "            \"timestamps\": torch.from_numpy(time_elapsed_timestamps),\n",
    "            \"prior_q_times\": torch.from_numpy(prior_q_times),\n",
    "            \"prior_q_explanation\": torch.from_numpy(prior_q_explanation).long(),\n",
    "            \"length\": window_size,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# The collate function is used to merge individual data samples into a batch\n",
    "# It handles the padding aspect\n",
    "def collate_fn(batch):\n",
    "    # collate lenghts into 1D tensor\n",
    "    items = {\"length\": torch.tensor([batch_item[\"length\"] for batch_item in batch])}\n",
    "\n",
    "    # find shape that the batch will have\n",
    "    max_length = items[\"length\"].max()\n",
    "    num_items = len(batch)\n",
    "\n",
    "    # padding list\n",
    "    for (key, padding) in [\n",
    "        (\"parts\", 0),\n",
    "        (\"content_ids\", 13942),\n",
    "        (\"answered_correctly\", 3),\n",
    "        (\"answers\", 3),\n",
    "        (\"timestamps\", 0.0),  # note timestamps isnt an embedding\n",
    "        (\"tags\", 188),\n",
    "        (\"prior_q_times\", 0),\n",
    "        (\"prior_q_explanation\", 0),\n",
    "    ]:\n",
    "        items[key] = pad_sequence(\n",
    "            [batch_item[key] for batch_item in batch],\n",
    "            batch_first=False,\n",
    "            padding_value=padding,\n",
    "        )\n",
    "\n",
    "    # mask to weight loss by (S, N)\n",
    "    items[\"loss_mask\"] = (\n",
    "        (\n",
    "            torch.arange(max_length).expand(num_items, max_length)\n",
    "            < items[\"length\"].unsqueeze(1)\n",
    "        )\n",
    "        .transpose(1, 0)\n",
    "        .float()\n",
    "    )\n",
    "    items[\"loss_mask\"] *= items[\"answered_correctly\"] != 4  # mask the lectures\n",
    "    items[\"answered_correctly\"] = items[\"answered_correctly\"].float()\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "max_window_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset + Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.44 s, sys: 2.19 s, total: 5.63 s\n",
      "Wall time: 5.66 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101230332"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Create Dataset with all rows\n",
    "user_mapping = train.user_id.values\n",
    "user_history = train.groupby(\"user_id\").cumcount().values + 1\n",
    "\n",
    "dataset = RIIDDataset(\n",
    "    user_mapping, user_history, hdf5_file=\"feats.h5\", window_size=100, use_cache=True\n",
    ")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.45 ms, sys: 1.25 s, total: 1.25 s\n",
      "Wall time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv_train_ids = pd.read_pickle(f\"cv5_train.pickle\").row_id.values\n",
    "cv_valid_ids = pd.read_pickle(f\"cv5_valid.pickle\").row_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 s, sys: 1.34 s, total: 21.8 s\n",
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question_ids = train[~train.content_type_id].row_id.values\n",
    "q_train_indices = np.intersect1d(question_ids, cv_train_ids)\n",
    "q_valid_indices = np.intersect1d(question_ids, cv_valid_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# Init DataLoader from RIIID Dataset subset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=Subset(dataset, cv_train_ids),\n",
    "    batch_size=batch_size,\n",
    "    sampler=torch.utils.data.SubsetRandomSampler(q_train_indices), # sampler to ignore lectures\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=6,\n",
    "    pin_memory=torch.cuda.is_available(),  # if GPU then pin memory for perf\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=Subset(dataset, cv_valid_ids),\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    sampler=torch.utils.data.SubsetRandomSampler(q_valid_indices), # sampler to ignore lectures\n",
    "    num_workers=6,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.core.decorators import auto_move_data\n",
    "from pytorch_lightning.metrics.functional.classification import auroc\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, sequence_length):\n",
    "        # returns embeds (sequence_length, 1, d_model)\n",
    "        return self.pe[:sequence_length, :]\n",
    "\n",
    "\n",
    "class RIIDDTransformerModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=0.001,\n",
    "        n_content_id=13943,  # number of different contents = 13942 + 1 (for padding)\n",
    "        n_part=8,  # number of different parts = 7 + 1 (for padding)\n",
    "        n_tags=189,  # number of different tags = 188 + 1 (for padding)\n",
    "        n_correct=5,  # 0,1 (false, true), 2 (start token), 3 (padding), 4 (lecture)\n",
    "        emb_dim=64,  # embedding dimension\n",
    "        dropout=0.1,\n",
    "        n_heads: int = 1,\n",
    "        n_encoder_layers: int = 2,\n",
    "        n_decoder_layers: int = 2,\n",
    "        dim_feedforward: int = 256,\n",
    "        activation: str = \"relu\",\n",
    "        max_window_size=100,\n",
    "        use_prior_q_times=False,\n",
    "        use_prior_q_explanation=False,\n",
    "    ):\n",
    "        super(RIIDDTransformerModel, self).__init__()\n",
    "        self.model_type = \"RiiidTransformer\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_window_size = max_window_size\n",
    "\n",
    "        self.use_prior_q_times = use_prior_q_times\n",
    "        self.use_prior_q_explanation = use_prior_q_explanation\n",
    "\n",
    "        # save params of models to yml\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        #### EXERCISE SEQUENCE\n",
    "        self.embed_content_id = nn.Embedding(n_content_id, emb_dim, padding_idx=13942)\n",
    "        self.embed_parts = nn.Embedding(n_part, emb_dim, padding_idx=0)\n",
    "        self.embed_tags = nn.Embedding(n_tags, emb_dim, padding_idx=188)\n",
    "\n",
    "        # exercise weights to weight the mean embeded excercise embeddings\n",
    "        self.exercise_weights = torch.nn.Parameter(torch.tensor([0.35, 0.55, 0.1]))\n",
    "\n",
    "        ### RESPONSE SEQUENCE (1st time stamp of sequence is useless)\n",
    "        self.embed_answered_correctly = nn.Embedding(\n",
    "            n_correct, emb_dim, padding_idx=3\n",
    "        )  # 2 + 1 for start token + 1 for padding_idn_inputs\n",
    "\n",
    "        self.embed_timestamps = nn.Linear(1, emb_dim)\n",
    "\n",
    "        # embed prior q time and q explanation\n",
    "        self.embed_prior_q_time = nn.Linear(1, emb_dim)\n",
    "        self.embed_prior_q_explanation = nn.Embedding(2, emb_dim)\n",
    "\n",
    "        # response weights to weight the mean embeded response embeddings\n",
    "        w = [0.5, 0.5]\n",
    "        if use_prior_q_times:\n",
    "            w.append(0.5)\n",
    "        if use_prior_q_explanation:\n",
    "            w.append(0.5)\n",
    "\n",
    "        self.response_weights = torch.nn.Parameter(torch.tensor([w]))\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=n_heads,\n",
    "            num_encoder_layers=n_encoder_layers,\n",
    "            num_decoder_layers=n_decoder_layers,\n",
    "            dropout=dropout,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "        self.out_linear = nn.Linear(emb_dim, 2)\n",
    "        init_weights(self)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = (\n",
    "            mask.float()\n",
    "            .masked_fill(mask == 0, float(\"-inf\"))\n",
    "            .masked_fill(mask == 1, float(0.0))\n",
    "        )\n",
    "        return mask\n",
    "\n",
    "    def get_random_steps(self, lengths, max_steps=10):\n",
    "        \"\"\"\n",
    "        for x return integer between 1 - 10 or \n",
    "        between 1 - x if x < 10\n",
    "        \"\"\"\n",
    "        m = torch.distributions.uniform.Uniform(\n",
    "            0,\n",
    "            (\n",
    "                torch.minimum(\n",
    "                    torch.ones(lengths.shape, device=self.device) * 10, lengths\n",
    "                )\n",
    "            ).float(),\n",
    "        )\n",
    "        return torch.floor(m.sample()).long() + 1\n",
    "\n",
    "    def get_random_lengths(self, lengths):\n",
    "        # gets random new lengths\n",
    "        m = torch.distributions.uniform.Uniform(0, lengths.float())\n",
    "        return torch.floor(m.sample()).long() + 1\n",
    "\n",
    "    def randomize_evaluation_step(self, batch, max_steps=10):\n",
    "        # randomize new lengths (for where start token is present)\n",
    "        # batch[\"length\"] = torch.where(\n",
    "        #     batch[\"answers\"][0, :] == 2,\n",
    "        #     self.get_random_lengths(batch[\"length\"]),\n",
    "        #     batch[\"length\"],\n",
    "        # )\n",
    "        # randomize number of steps based on new random lengths\n",
    "        batch[\"steps\"] = self.get_random_steps(batch[\"length\"], max_steps=max_steps)\n",
    "        return batch\n",
    "\n",
    "    @auto_move_data\n",
    "    def forward(\n",
    "        self,\n",
    "        content_ids,\n",
    "        parts,\n",
    "        answers,\n",
    "        tags,\n",
    "        timestamps,\n",
    "        prior_q_times,\n",
    "        prior_q_explanation,\n",
    "    ):\n",
    "        # content_ids: (Source Sequence Length, Number of samples, Embedding)\n",
    "        # tgt: (Target Sequence Length,Number of samples, Embedding)\n",
    "\n",
    "        # if data is flat then expand to get Batch dim\n",
    "        if len(content_ids.shape) == 1:\n",
    "            content_ids = content_ids.unsqueeze(1)\n",
    "            parts = parts.unsqueeze(1)\n",
    "            answers = answers.unsqueeze(1)\n",
    "            tags = tags.unsqueeze(1)\n",
    "            timestamps = timestamps.unsqueeze(1)\n",
    "            prior_q_times = prior_q_times.unsqueeze(1)\n",
    "            prior_q_explanation = prior_q_explanation.unsqueeze(1)\n",
    "\n",
    "        sequence_length = content_ids.shape[0]\n",
    "\n",
    "        # sequence that will go into encoder\n",
    "        embeded_content = self.embed_content_id(content_ids)\n",
    "        embeded_parts = self.embed_parts(parts)\n",
    "        embeded_tags = self.embed_tags(tags).sum(dim=2)\n",
    "        e_w = F.softmax(self.exercise_weights, dim=0)\n",
    "\n",
    "        embeded_exercise_sequence = (\n",
    "            torch.stack([embeded_content, embeded_parts, embeded_tags], dim=3) * e_w\n",
    "        ).sum(dim=3)\n",
    "\n",
    "        # sequence that will go into decoder\n",
    "        embeded_responses = self.embed_answered_correctly(answers)\n",
    "        embeded_timestamps = self.embed_timestamps(timestamps.unsqueeze(2))\n",
    "        embeded_q_times = self.embed_prior_q_time(prior_q_times.unsqueeze(2))\n",
    "        embeded_q_explanation = self.embed_prior_q_explanation(prior_q_explanation)\n",
    "        r_w = F.softmax(self.response_weights, dim=0)\n",
    "\n",
    "        exercise_sequence_components = [embeded_responses, embeded_timestamps]\n",
    "        if self.use_prior_q_times:\n",
    "            # zero embedding - if start token\n",
    "            embeded_q_times[0,torch.where(answers[0,:] == 2)[0],:] = 0\n",
    "            exercise_sequence_components.append(embeded_q_times)\n",
    "        if self.use_prior_q_explanation:\n",
    "            # zero embedding - if start token\n",
    "            embeded_q_explanation[0,torch.where(answers[0,:] == 2)[0],:] = 0\n",
    "            exercise_sequence_components.append(embeded_q_explanation)\n",
    "\n",
    "        embeded_response = (torch.stack(exercise_sequence_components, dim=3) * r_w).sum(\n",
    "            dim=3\n",
    "        )\n",
    "\n",
    "        # adding positional vector\n",
    "        embedded_positions = self.pos_encoder(sequence_length)\n",
    "        embeded_responses = embeded_responses + embedded_positions\n",
    "        embeded_exercise_sequence = embeded_exercise_sequence + embedded_positions\n",
    "\n",
    "        # mask of shape S x S -> prevents attention looking forward\n",
    "        top_right_attention_mask = self.generate_square_subsequent_mask(\n",
    "            sequence_length\n",
    "        ).type_as(embeded_exercise_sequence)\n",
    "\n",
    "        output = self.transformer(\n",
    "            embeded_exercise_sequence,\n",
    "            embeded_responses,\n",
    "            tgt_mask=top_right_attention_mask,  # (S,S)\n",
    "            src_mask=top_right_attention_mask,  # (T,T)\n",
    "        )\n",
    "\n",
    "        output = self.out_linear(output)\n",
    "        return F.softmax(output, dim=2)[:, :, 1]\n",
    "\n",
    "    def process_batch_step(self, batch):\n",
    "        # return result\n",
    "        return self(\n",
    "            batch[\"content_ids\"],\n",
    "            batch[\"parts\"],\n",
    "            batch[\"answers\"],\n",
    "            batch[\"tags\"],\n",
    "            batch[\"timestamps\"],\n",
    "            batch[\"prior_q_times\"],\n",
    "            batch[\"prior_q_explanation\"],\n",
    "        )\n",
    "\n",
    "    @auto_move_data\n",
    "    def predict_n_steps(self, batch, steps, return_all_preds=False):\n",
    "        \"\"\"\n",
    "        Predicts n steps for all items in batch and return predictions\n",
    "        only for those steps (flattened)\n",
    "        steps: tensor of length B where each item is the number of steps that need to be taken\n",
    "        \"\"\"\n",
    "        n_users = batch[\"content_ids\"].shape[1]\n",
    "        lengths = batch[\"length\"]\n",
    "\n",
    "        users = torch.arange(n_users)\n",
    "\n",
    "        user_indexes = []\n",
    "        sequence_indexes = []\n",
    "\n",
    "        for i in range(steps.max().int(), 0, -1):\n",
    "            preds = self.process_batch_step(batch)\n",
    "\n",
    "            sequence_indexes_at_i = lengths[steps >= i] - i\n",
    "            user_indexes_at_i = users[steps >= i]\n",
    "\n",
    "            # get index for which to update the answers\n",
    "            # since answers is shifted we want to map preds 0..98 -> answers 1:99\n",
    "            answers_idx = torch.where(sequence_indexes_at_i + 1 != seq_length)\n",
    "            a_seq_idx = sequence_indexes_at_i[answers_idx] + 1\n",
    "            u_seq_idx = user_indexes_at_i[answers_idx]\n",
    "\n",
    "            # set answer to either 0 or 1 if not lecture\n",
    "            batch[\"answers\"][a_seq_idx, u_seq_idx] = torch.where(\n",
    "                batch[\"answers\"][a_seq_idx, u_seq_idx] != 4,\n",
    "                (preds[sequence_indexes_at_i[answers_idx], u_seq_idx] > 0.5).long(),\n",
    "                batch[\"answers\"][a_seq_idx, u_seq_idx],\n",
    "            )\n",
    "\n",
    "            user_indexes.append(user_indexes_at_i)\n",
    "            sequence_indexes.append(sequence_indexes_at_i)\n",
    "\n",
    "        if return_all_preds:\n",
    "            return preds\n",
    "\n",
    "        user_indexes = torch.cat(user_indexes)\n",
    "        sequence_indexes = torch.cat(sequence_indexes)\n",
    "\n",
    "        return (\n",
    "            preds[sequence_indexes, user_indexes],\n",
    "            batch[\"row_ids\"][sequence_indexes, user_indexes],\n",
    "        )\n",
    "\n",
    "    @auto_move_data\n",
    "    def predict_fast_single_user(\n",
    "        self, content_ids, parts, answers, tags, timestamps, n=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Predicts n steps for a single user in batch and return predictions\n",
    "        only for those steps (flattened)\n",
    "        \"\"\"\n",
    "        length = len(content_ids)\n",
    "        out_predictions = torch.zeros(n, device=self.device)\n",
    "        for i in range(n, 0, -1):\n",
    "            preds = self(content_ids, parts, answers, tags, timestamps)\n",
    "            out_predictions[n - i] = preds[length - i, 0]\n",
    "\n",
    "            # answers are shifted (start token) so need + 1\n",
    "            answer_idx = length - i + 1\n",
    "            # don't update if at end of answers\n",
    "            if answer_idx + 1 < len(answers):\n",
    "                # don't update if true is lecture\n",
    "                if answers[answer_idx] != 4:\n",
    "                    answers[answer_idx] = (preds[length - i, 0] > 0.5).long()\n",
    "\n",
    "        return out_predictions\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        result = self.process_batch_step(batch)\n",
    "        loss = F.binary_cross_entropy(\n",
    "            result, batch[\"answered_correctly\"], weight=batch[\"loss_mask\"]\n",
    "        )\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validate_n_steps(self, batch, max_steps=10):\n",
    "        \"\"\"\n",
    "        Predicts max_steps steps for all items in batch and return predictions\n",
    "        only for those steps (flattened)\n",
    "        steps: tensor of length B where each item is the number of steps that need to be taken\n",
    "        \"\"\"\n",
    "        n_users = batch[\"content_ids\"].shape[1]\n",
    "        seq_length = batch[\"answers\"].shape[0]\n",
    "        lengths = batch[\"length\"]\n",
    "        steps = batch[\"steps\"]\n",
    "        users = torch.arange(n_users)\n",
    "        user_indexes = []\n",
    "        sequence_indexes = []\n",
    "        for i in range(steps.max().int(), 0, -1):\n",
    "            preds = self.process_batch_step(batch)\n",
    "            sequence_indexes_at_i = lengths[steps >= i] - i\n",
    "            user_indexes_at_i = users[steps >= i]\n",
    "\n",
    "            # get index for which to update the answers\n",
    "            # since answers is shifted we want to map preds 0..98 -> answers 1:99\n",
    "            answers_idx = torch.where(sequence_indexes_at_i + 1 != seq_length)\n",
    "            a_seq_idx = sequence_indexes_at_i[answers_idx] + 1\n",
    "            u_seq_idx = user_indexes_at_i[answers_idx]\n",
    "\n",
    "            # set answer to either 0 or 1 if not lecture\n",
    "            batch[\"answers\"][a_seq_idx, u_seq_idx] = torch.where(\n",
    "                batch[\"answers\"][a_seq_idx, u_seq_idx] != 4,\n",
    "                (preds[sequence_indexes_at_i[answers_idx], u_seq_idx] > 0.5).long(),\n",
    "                batch[\"answers\"][a_seq_idx, u_seq_idx],\n",
    "            )\n",
    "\n",
    "            user_indexes.append(user_indexes_at_i)\n",
    "            sequence_indexes.append(sequence_indexes_at_i)\n",
    "\n",
    "        user_indexes = torch.cat(user_indexes)\n",
    "        sequence_indexes = torch.cat(sequence_indexes)\n",
    "        return (preds, sequence_indexes, user_indexes)\n",
    "\n",
    "    def val_test_step(self, batch, log_as=\"val\"):\n",
    "        batch = self.randomize_evaluation_step(batch)\n",
    "\n",
    "        result, sequence_indexes, user_indexes = self.validate_n_steps(batch)\n",
    "\n",
    "        step_mask = torch.zeros(batch[\"loss_mask\"].shape, device=self.device)\n",
    "        step_mask[sequence_indexes, user_indexes] = 1\n",
    "\n",
    "        batch[\"loss_mask\"] *= step_mask\n",
    "\n",
    "        loss = F.binary_cross_entropy(\n",
    "            result, batch[\"answered_correctly\"], weight=batch[\"loss_mask\"]\n",
    "        )\n",
    "        self.log(f\"{log_as}_loss_step\", loss)\n",
    "        select_mask = batch[\"loss_mask\"] > 0\n",
    "        positions = torch.cat(\n",
    "            result.shape[1] * [torch.arange(result.shape[0]).unsqueeze(1)], dim=1\n",
    "        )\n",
    "        return (\n",
    "            torch.masked_select(result, select_mask),\n",
    "            torch.masked_select(batch[\"answered_correctly\"], select_mask),\n",
    "            torch.masked_select(positions, select_mask),\n",
    "        )\n",
    "\n",
    "    def val_test_epoch_end(self, outputs, log_as=\"val\"):\n",
    "        y_pred = torch.cat([out[0] for out in outputs], dim=0)\n",
    "        y = torch.cat([out[1] for out in outputs], dim=0)\n",
    "        pos = torch.cat([out[2] for out in outputs], dim=0)\n",
    "        auc = auroc(y_pred, y)\n",
    "\n",
    "        # Calculate accuracy per position\n",
    "        M = torch.zeros(pos.max() + 1, len(y), device=self.device)\n",
    "        M[pos, torch.arange(len(y))] = 1\n",
    "        M = torch.nn.functional.normalize(M, p=1, dim=1)\n",
    "        acc_per_position = torch.mm(\n",
    "            M, ((y_pred > 0.5) == y).float().unsqueeze(1)\n",
    "        ).flatten()\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        sns.regplot(\n",
    "            y=acc_per_position.cpu(), x=torch.arange(len(acc_per_position)).cpu(), ax=ax\n",
    "        )\n",
    "        ax.set_ylim(0.5, 1)\n",
    "        ax.set_xlim(0, len(acc_per_position) - 1)\n",
    "        ax.set_ylabel(\"acc\")\n",
    "        ax.set_xlabel(\"position\")\n",
    "        if log_as == \"val\":\n",
    "            self.log(f\"avg_{log_as}_auc\", auc, prog_bar=True)\n",
    "            self.logger.experiment.add_figure(\n",
    "                f\"{log_as}_acc_per_pos\", fig, global_step=self.current_epoch\n",
    "            )\n",
    "        else:\n",
    "            self.log(f\"avg_{log_as}_auc\", auc)\n",
    "            self.logger.experiment.add_figure(\n",
    "                f\"{log_as}_acc_per_pos\", fig, global_step=1\n",
    "            )\n",
    "\n",
    "    def validation_step(self, batch, batch_nb, dataset_nb=None):\n",
    "        return self.val_test_step(batch, log_as=\"val\")\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.val_test_epoch_end(outputs, log_as=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_nb, dataset_nb=None):\n",
    "        return self.val_test_step(batch, log_as=\"test\")\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        self.val_test_epoch_end(outputs, log_as=\"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode=\"max\", patience=10\n",
    "            ),\n",
    "            \"monitor\": \"avg_val_auc\",\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1000,\n",
    "            \"strict\": True,\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                      | Type               | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | embed_content_id          | Embedding          | 892 K \n",
      "1 | embed_parts               | Embedding          | 512   \n",
      "2 | embed_tags                | Embedding          | 12.1 K\n",
      "3 | embed_answered_correctly  | Embedding          | 320   \n",
      "4 | embed_timestamps          | Linear             | 128   \n",
      "5 | embed_prior_q_time        | Linear             | 128   \n",
      "6 | embed_prior_q_explanation | Embedding          | 128   \n",
      "7 | pos_encoder               | PositionalEncoding | 0     \n",
      "8 | transformer               | Transformer        | 233 K \n",
      "9 | out_linear                | Linear             | 130   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc817055700>\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc817055700>\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc817055700>\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc817055700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/multiprocessing/process.py\", line 147, in join\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    self._shutdown_workers()\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/multiprocessing/process.py\", line 147, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/multiprocessing/process.py\", line 147, in join\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/multiprocessing/process.py\", line 147, in join\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc817055700>\n",
      "AssertionError: can only join a child process\n",
      "Traceback (most recent call last):\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "AssertionError: can only join a child process\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc817055700>\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/multiprocessing/process.py\", line 147, in join\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    self._shutdown_workers()\n",
      "AssertionError: can only join a child process\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/gautier/miniconda3/envs/kaggle/lib/python3.8/multiprocessing/process.py\", line 147, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ab44cbda434121a748980b00384085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gautier/miniconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "\n",
    "# Params\n",
    "learning_rate = 0.005  # 0.0001\n",
    "emb_dim = 64  # 256\n",
    "dropout = 0.0\n",
    "n_heads = 2  # 2\n",
    "n_encoder_layers = 2\n",
    "n_decoder_layers = 2\n",
    "dim_feedforward = 256\n",
    "use_prior_q_times = False\n",
    "use_prior_q_explanation = False\n",
    "\n",
    "# Init our model\n",
    "model = RIIDDTransformerModel(\n",
    "    learning_rate=learning_rate,\n",
    "    emb_dim=emb_dim,  # embedding dimension - this is for everything\n",
    "    dropout=dropout,\n",
    "    n_heads=n_heads,\n",
    "    n_encoder_layers=n_encoder_layers,\n",
    "    n_decoder_layers=n_decoder_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    max_window_size=max_window_size,\n",
    "    use_prior_q_times=use_prior_q_times,\n",
    "    use_prior_q_explanation=use_prior_q_explanation,\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"base_new_dataset\",)\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=500,\n",
    "    progress_bar_refresh_rate=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"avg_val_auc\", patience=20, mode=\"max\"),\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"avg_val_auc\",\n",
    "            filename=\"{epoch}-{val_loss_step:.2f}-{avg_val_auc:.2f}\",\n",
    "            mode=\"max\",\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ],\n",
    "    logger=logger,\n",
    "    val_check_interval=1000, # check validation every 1000 step\n",
    "    limit_val_batches=0.10, # run through only 25% of val every time\n",
    ")\n",
    "\n",
    "# Train the model ⚡\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloaders=[val_loader],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best\n",
    "model = RIIDDTransformerModel.load_from_checkpoint(\n",
    "    \"lightning_logs/weighted_sampling_h_4/version_0/checkpoints/epoch=38-val_loss_step=0.03-avg_val_auc=0.81.ckpt\"\n",
    ")\n",
    "\n",
    "model.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Kaggle Sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceRIIDDataset(Dataset):\n",
    "    \"\"\"RIID dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, hdf5_file=\"feats.h5\", window_size=100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_mapping (np.array): array of all unique user ids \n",
    "            hdf5_file (string): location of hf5 feats file\n",
    "        \"\"\"\n",
    "        self.hdf5_file = hdf5_file\n",
    "        self.max_window_size = window_size\n",
    "        self.cache = {}\n",
    "\n",
    "    def open_hdf5(self):\n",
    "        # opens the h5py file\n",
    "        self.f = h5py.File(self.hdf5_file, \"r\")\n",
    "\n",
    "    def __len__(self):\n",
    "        if not hasattr(self, \"f\"):\n",
    "            self.open_hdf5()\n",
    "        return len(self.f.keys())\n",
    "\n",
    "    def load_user_into_cache(self, user_id):\n",
    "        \"\"\"\n",
    "        add a user to self.cache\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, \"f\"):\n",
    "            self.open_hdf5()\n",
    "\n",
    "        if f\"{user_id}\" in self.f:\n",
    "            content_ids = np.array(self.f[f\"{user_id}/content_ids\"], dtype=np.int64)\n",
    "            answers = np.array(self.f[f\"{user_id}/answered_correctly\"], dtype=np.int64)\n",
    "            timestamps = np.array(self.f[f\"{user_id}/timestamps\"], dtype=np.int64)\n",
    "            self.cache[user_id] = {\n",
    "                \"content_ids\": content_ids.astype(np.int64),\n",
    "                \"answers\": answers,\n",
    "                \"timestamps\": timestamps,\n",
    "                \"row_ids\": np.zeros(len(timestamps)),\n",
    "                \"steps\": 0,\n",
    "            }\n",
    "        else:\n",
    "            self.cache[user_id] = {\n",
    "                \"content_ids\": np.array([], dtype=\"int64\"),\n",
    "                \"timestamps\": np.array([], dtype=\"float32\"),\n",
    "                \"answers\": np.array([], dtype=\"int64\"),\n",
    "                \"row_ids\": np.array([], dtype=\"int64\"),\n",
    "                \"steps\": 0,\n",
    "            }\n",
    "\n",
    "    def update_user_rows(self, user_rows):\n",
    "        if not hasattr(self, \"f\"):\n",
    "            self.open_hdf5()\n",
    "\n",
    "        user_id = user_rows.user_id.values[0]\n",
    "        num_rows = len(user_rows)\n",
    "        new_content_ids = user_rows.content_id.values\n",
    "        new_timestamps = user_rows.timestamp.values\n",
    "        new_answered_correctly = (\n",
    "            user_rows.answered_correctly.values\n",
    "        )  # should be 3 (3) for every question and 4 for lectures\n",
    "        new_row_ids = user_rows.row_id.values\n",
    "\n",
    "        if user_id not in self.cache:\n",
    "            self.load_user_into_cache(user_id)\n",
    "\n",
    "        self.cache[user_id][\"content_ids\"] = np.concatenate(\n",
    "            [self.cache[user_id][\"content_ids\"], new_content_ids]\n",
    "        )[-(self.max_window_size + 1) :]\n",
    "\n",
    "        self.cache[user_id][\"timestamps\"] = np.concatenate(\n",
    "            [self.cache[user_id][\"timestamps\"], new_timestamps]\n",
    "        )[-(self.max_window_size + 1) :]\n",
    "\n",
    "        self.cache[user_id][\"answers\"] = np.concatenate(\n",
    "            [self.cache[user_id][\"answers\"], new_answered_correctly]\n",
    "        )[-(self.max_window_size + 1) :]\n",
    "\n",
    "        self.cache[user_id][\"row_ids\"] = np.concatenate(\n",
    "            [self.cache[user_id][\"row_ids\"], new_row_ids]\n",
    "        )[-(self.max_window_size + 1) :]\n",
    "\n",
    "        self.cache[user_id][\"steps\"] = len(new_row_ids)\n",
    "\n",
    "    def update_answered_correctly(self, answered_correctly_rows):\n",
    "        user_id = answered_correctly_rows.name\n",
    "        new_answered_correctly = (\n",
    "            answered_correctly_rows.values\n",
    "        )  # this is only the answers\n",
    "        \n",
    "        n = self.cache[user_id][\"steps\"]\n",
    "        l = len(self.cache[user_id][\"answers\"])\n",
    "        idxs = np.where(self.cache[user_id][\"answers\"][-n:] != 4)[0] + l - n\n",
    "\n",
    "        self.cache[user_id][\"answers\"][idxs] = new_answered_correctly\n",
    "        self.cache[user_id][\"steps\"] = 0\n",
    "\n",
    "\n",
    "    def __getitem__(self, user_id):\n",
    "        if user_id not in self.cache:\n",
    "            self.load_user_into_cache(user_id)\n",
    "\n",
    "        length = len(self.cache[user_id][\"content_ids\"])\n",
    "        window_size = min(self.max_window_size, length)\n",
    "\n",
    "        content_ids = self.cache[user_id][\"content_ids\"][-window_size:]\n",
    "\n",
    "        answers = self.cache[user_id][\"answers\"][-window_size:]\n",
    "        timestamps = self.cache[user_id][\"timestamps\"][-window_size:]\n",
    "        row_ids = self.cache[user_id][\"row_ids\"][-window_size:]\n",
    "\n",
    "        # convert timestamps to time elapsed\n",
    "        time_elapsed_timestamps = get_time_elapsed_from_timestamp(timestamps)\n",
    "\n",
    "        # get tags\n",
    "        tags = questions_lectures_tags[content_ids, :].astype(np.int64)\n",
    "\n",
    "        # get parts\n",
    "        parts = questions_lectures_parts[content_ids].astype(np.int64)\n",
    "\n",
    "        # shift by one the answered_correctly sequence\n",
    "        answers = np.roll(answers, 1)\n",
    "\n",
    "        if length > self.max_window_size:\n",
    "            answers[0] = self.cache[user_id][\"answers\"][-window_size - 1]\n",
    "        else:\n",
    "            answers[0] = 2\n",
    "\n",
    "        return {\n",
    "            \"parts\": torch.from_numpy(parts).long(),\n",
    "            \"tags\": torch.from_numpy(tags).long(),\n",
    "            \"content_ids\": torch.from_numpy(content_ids).long(),\n",
    "            \"answers\": torch.from_numpy(answers).long(),\n",
    "            \"timestamps\": torch.from_numpy(time_elapsed_timestamps).float(),\n",
    "            \"length\": window_size,\n",
    "            \"row_ids\": torch.from_numpy(row_ids).int(),\n",
    "            \"steps\": self.cache[user_id][\"steps\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# The collate function is used to merge individual data samples into a batch\n",
    "# It handles the padding aspect\n",
    "def inference_collate_fn(batch):\n",
    "    # collate lenghts into 1D tensor\n",
    "    items = {\n",
    "        \"length\": torch.tensor([batch_item[\"length\"] for batch_item in batch]),\n",
    "        \"steps\": torch.tensor([batch_item[\"steps\"] for batch_item in batch]),\n",
    "    }\n",
    "\n",
    "    # find shape that the batch will have\n",
    "    max_length = items[\"length\"].max()\n",
    "    num_items = len(batch)\n",
    "    if num_items > 1:\n",
    "        # padding list\n",
    "        for (key, padding) in [\n",
    "            (\"parts\", 0),\n",
    "            (\"content_ids\", 13942),\n",
    "            (\"answers\", 3),\n",
    "            (\"timestamps\", 0.0),  # note timestamps isnt an embedding\n",
    "            (\"tags\", 188),\n",
    "            (\"row_ids\", 0),\n",
    "        ]:\n",
    "            items[key] = pad_sequence(\n",
    "                [batch_item[key] for batch_item in batch],\n",
    "                batch_first=False,\n",
    "                padding_value=padding,\n",
    "            )\n",
    "    else:\n",
    "        for key in [\"parts\", \"content_ids\", \"answers\", \"timestamps\", \"tags\", \"row_ids\"]:\n",
    "            items[key] = batch[0][key].unsqueeze(1)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# model = RIIDDTransformerModel.load_from_checkpoint(\n",
    "#     \"lightning_logs/lecture_model/version_10/checkpoints/epoch=69-val_loss_step=0.03-avg_val_auc=0.79.ckpt\"\n",
    "# )\n",
    "model.freeze()\n",
    "model.cuda()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 483 ms, total: 1min 1s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_groups = np.array_split(valid, len(valid) // 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>user_answer</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "      <th>answered_correctly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16662</th>\n",
       "      <td>16662</td>\n",
       "      <td>1783912</td>\n",
       "      <td>157207</td>\n",
       "      <td>2064</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014443</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16663</th>\n",
       "      <td>16663</td>\n",
       "      <td>1783912</td>\n",
       "      <td>157207</td>\n",
       "      <td>2065</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014443</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16664</th>\n",
       "      <td>16664</td>\n",
       "      <td>1818014</td>\n",
       "      <td>157207</td>\n",
       "      <td>1278</td>\n",
       "      <td>False</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16665</th>\n",
       "      <td>16665</td>\n",
       "      <td>1831029</td>\n",
       "      <td>157207</td>\n",
       "      <td>175</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0.056667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16666</th>\n",
       "      <td>16666</td>\n",
       "      <td>1847871</td>\n",
       "      <td>157207</td>\n",
       "      <td>7876</td>\n",
       "      <td>False</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16667</th>\n",
       "      <td>16667</td>\n",
       "      <td>1860463</td>\n",
       "      <td>157207</td>\n",
       "      <td>7900</td>\n",
       "      <td>False</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20077</th>\n",
       "      <td>20077</td>\n",
       "      <td>24747352314</td>\n",
       "      <td>238966</td>\n",
       "      <td>400</td>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.043333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20078</th>\n",
       "      <td>20078</td>\n",
       "      <td>24747378068</td>\n",
       "      <td>238966</td>\n",
       "      <td>10685</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20079</th>\n",
       "      <td>20079</td>\n",
       "      <td>24747405783</td>\n",
       "      <td>238966</td>\n",
       "      <td>737</td>\n",
       "      <td>False</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20080</th>\n",
       "      <td>20080</td>\n",
       "      <td>24747427874</td>\n",
       "      <td>238966</td>\n",
       "      <td>1078</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.063333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       row_id    timestamp  user_id  content_id  content_type_id  \\\n",
       "16662   16662      1783912   157207        2064            False   \n",
       "16663   16663      1783912   157207        2065            False   \n",
       "16664   16664      1818014   157207        1278            False   \n",
       "16665   16665      1831029   157207         175            False   \n",
       "16666   16666      1847871   157207        7876            False   \n",
       "16667   16667      1860463   157207        7900            False   \n",
       "20077   20077  24747352314   238966         400            False   \n",
       "20078   20078  24747378068   238966       10685            False   \n",
       "20079   20079  24747405783   238966         737            False   \n",
       "20080   20080  24747427874   238966        1078            False   \n",
       "\n",
       "       task_container_id  user_answer  prior_question_elapsed_time  \\\n",
       "16662                 27            1                     0.014443   \n",
       "16663                 27            1                     0.014443   \n",
       "16664                 28            3                     0.020000   \n",
       "16665                 29            2                     0.056667   \n",
       "16666                 30            3                     0.006667   \n",
       "16667                 31            0                     0.010000   \n",
       "20077                 17            1                     0.043333   \n",
       "20078                 18            0                     0.046667   \n",
       "20079                 19            3                     0.060000   \n",
       "20080                 20            0                     0.063333   \n",
       "\n",
       "       prior_question_had_explanation  answered_correctly  \n",
       "16662                               1                 0.5  \n",
       "16663                               1                 0.5  \n",
       "16664                               1                 0.5  \n",
       "16665                               1                 0.5  \n",
       "16666                               1                 0.5  \n",
       "16667                               1                 0.5  \n",
       "20077                               0                 0.5  \n",
       "20078                               1                 0.5  \n",
       "20079                               1                 0.5  \n",
       "20080                               1                 0.5  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = InferenceRIIDDataset(hdf5_file=\"train_feats.h5\",)\n",
    "\n",
    "\n",
    "def predict_for_df(df):\n",
    "    # select questions only\n",
    "    unique_users_with_questions = df[~df.content_type_id].user_id.unique()\n",
    "    df[\"answered_correctly\"] = 0.5  # set useless value for column\n",
    "\n",
    "    # batch with only lectures\n",
    "    if len(unique_users_with_questions) < 1:\n",
    "        pass  # return empy\n",
    "\n",
    "    # case of single user with questions in batch\n",
    "    elif len(unique_users_with_questions) == 1:\n",
    "        item = inference_dataset[unique_users_with_questions[0]].copy()\n",
    "        predictions = model.predict_fast_single_user(\n",
    "            item[\"content_ids\"],\n",
    "            item[\"parts\"],\n",
    "            item[\"answers\"],\n",
    "            item[\"tags\"],\n",
    "            item[\"timestamps\"],\n",
    "            n=item[\"steps\"],\n",
    "        ).cpu()\n",
    "        df.loc[\n",
    "            df.user_id == unique_users_with_questions[0], \"answered_correctly\"\n",
    "        ] = predictions.numpy()\n",
    "    # case of multiple users with questions in batch\n",
    "    else:\n",
    "        batch = inference_collate_fn(\n",
    "            [inference_dataset[u].copy() for u in unique_users_with_questions]\n",
    "        )\n",
    "        predictions, row_ids = model.predict_n_steps(batch, batch[\"steps\"])\n",
    "        df[\"answered_correctly\"] = df[\"row_id\"].map(\n",
    "            dict(zip(row_ids.cpu().numpy(), predictions.cpu().numpy()))\n",
    "        )\n",
    "    return df[~df.content_type_id]\n",
    "\n",
    "\n",
    "previous_test_df = None\n",
    "predictions = []\n",
    "row_ids = []\n",
    "for current_test in tqdm(valid_groups):\n",
    "    if previous_test_df is not None:\n",
    "        previous_test_df[~previous_test_df.content_type_id].groupby(\n",
    "            \"user_id\"\n",
    "        ).answered_correctly.apply(inference_dataset.update_answered_correctly)\n",
    "    # add current to cache\n",
    "    current_test = current_test.copy()\n",
    "    previous_test_df = current_test.copy()\n",
    "    current_test.drop(columns=[\"answered_correctly\"], inplace=True)\n",
    "\n",
    "    # preprocessing code heree\n",
    "    current_test = preprocess_df(current_test)\n",
    "    current_test[\n",
    "        [\"row_id\", \"user_id\", \"content_id\", \"timestamp\", \"answered_correctly\"]\n",
    "    ].groupby(\"user_id\").apply(\n",
    "        lambda user_rows: inference_dataset.update_user_rows(user_rows)\n",
    "    )\n",
    "\n",
    "    # your prediction code here\n",
    "    current_test = predict_for_df(current_test)\n",
    "    predictions.append(current_test[\"answered_correctly\"].values)\n",
    "    row_ids.append(current_test[\"row_id\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation auc: 0.7936167127055739\n"
     ]
    }
   ],
   "source": [
    "all_preds = np.concatenate(predictions)\n",
    "all_row_ids = np.concatenate(row_ids)\n",
    "df = pd.DataFrame({\"predictions\": all_preds, \"row_id\": all_row_ids})\n",
    "df = df.merge(valid[[\"row_id\", \"user_id\", \"answered_correctly\"]], on=\"row_id\", how=\"left\")\n",
    "print('validation auc:',roc_auc_score(df.answered_correctly.values, df.predictions.values))\n",
    "\n",
    "#validation auc: 0.7704724230724678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def roc_score_per_interaction(x):\n",
    "    try:\n",
    "        return roc_auc_score(x.answered_correctly.values, x.predictions.values)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "df[\"num_iteractions\"] = (df.groupby(\"user_id\").cumcount() // 10) * 10\n",
    "interactions_auc = df.groupby(\"num_iteractions\").apply(roc_score_per_interaction)\n",
    "interactions_auc = interactions_auc[~interactions_auc.isna()].copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.regplot(\n",
    "    y=interactions_auc.values,\n",
    "    x=interactions_auc.index,\n",
    "    line_kws={\"color\": \"red\", \"linewidth\": 2},\n",
    "    scatter_kws={\"s\": 3},\n",
    ")\n",
    "ax.set_ylim(0.5, 1)\n",
    "ax.set_xlim(0, interactions_auc.index.max())\n",
    "ax.set_ylabel(\"auc\")\n",
    "ax.set_xlabel(\"number of seen interactions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Iter_Valid(object):\n",
    "    def __init__(self, df, max_user=1000):\n",
    "        df = df.reset_index(drop=True)\n",
    "        self.df = df\n",
    "        self.user_answer = df[\"user_answer\"].astype(str).values\n",
    "        self.answered_correctly = df[\"answered_correctly\"].astype(str).values\n",
    "        df[\"prior_group_responses\"] = \"[]\"\n",
    "        df[\"prior_group_answers_correct\"] = \"[]\"\n",
    "        self.sample_df = df[df[\"content_type_id\"] == 0][[\"row_id\"]]\n",
    "        self.sample_df[\"answered_correctly\"] = 0\n",
    "        self.len = len(df)\n",
    "        self.user_id = df.user_id.values\n",
    "        self.task_container_id = df.task_container_id.values\n",
    "        self.content_type_id = df.content_type_id.values\n",
    "        self.max_user = max_user\n",
    "        self.current = 0\n",
    "        self.pre_user_answer_list = []\n",
    "        self.pre_answered_correctly_list = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def fix_df(self, user_answer_list, answered_correctly_list, pre_start):\n",
    "        df = self.df[pre_start : self.current].copy()\n",
    "        sample_df = self.sample_df[pre_start : self.current].copy()\n",
    "        df.loc[pre_start, \"prior_group_responses\"] = (\n",
    "            \"[\" + \",\".join(self.pre_user_answer_list) + \"]\"\n",
    "        )\n",
    "        df.loc[pre_start, \"prior_group_answers_correct\"] = (\n",
    "            \"[\" + \",\".join(self.pre_answered_correctly_list) + \"]\"\n",
    "        )\n",
    "        self.pre_user_answer_list = user_answer_list\n",
    "        self.pre_answered_correctly_list = answered_correctly_list\n",
    "        return df, sample_df\n",
    "\n",
    "    def __next__(self):\n",
    "        added_user = set()\n",
    "        pre_start = self.current\n",
    "        pre_added_user = -1\n",
    "        pre_task_container_id = -1\n",
    "\n",
    "        user_answer_list = []\n",
    "        answered_correctly_list = []\n",
    "        while self.current < self.len:\n",
    "            crr_user_id = self.user_id[self.current]\n",
    "            crr_task_container_id = self.task_container_id[self.current]\n",
    "            crr_content_type_id = self.content_type_id[self.current]\n",
    "            if crr_content_type_id == 1:\n",
    "                # no more than one task_container_id of \"questions\" from any single user\n",
    "                # so we only care for content_type_id == 0 to break loop\n",
    "                user_answer_list.append(self.user_answer[self.current])\n",
    "                answered_correctly_list.append(self.answered_correctly[self.current])\n",
    "                self.current += 1\n",
    "                continue\n",
    "            if crr_user_id in added_user and (\n",
    "                (crr_user_id != pre_added_user)\n",
    "                or (crr_task_container_id != pre_task_container_id)\n",
    "            ):\n",
    "                # known user(not prev user or differnt task container)\n",
    "                return self.fix_df(user_answer_list, answered_correctly_list, pre_start)\n",
    "            if len(added_user) == self.max_user:\n",
    "                if (\n",
    "                    crr_user_id == pre_added_user\n",
    "                    and crr_task_container_id == pre_task_container_id\n",
    "                ):\n",
    "                    user_answer_list.append(self.user_answer[self.current])\n",
    "                    answered_correctly_list.append(\n",
    "                        self.answered_correctly[self.current]\n",
    "                    )\n",
    "                    self.current += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    return self.fix_df(\n",
    "                        user_answer_list, answered_correctly_list, pre_start\n",
    "                    )\n",
    "            added_user.add(crr_user_id)\n",
    "            pre_added_user = crr_user_id\n",
    "            pre_task_container_id = crr_task_container_id\n",
    "            user_answer_list.append(self.user_answer[self.current])\n",
    "            answered_correctly_list.append(self.answered_correctly[self.current])\n",
    "            self.current += 1\n",
    "        if pre_start < self.current:\n",
    "            return self.fix_df(user_answer_list, answered_correctly_list, pre_start)\n",
    "        else:\n",
    "            raise StopIteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inference_dataset = InferenceRIIDDataset(hdf5_file=\"feats_train.h5\",)\n",
    "iter_test = Iter_Valid(valid,max_user=1000)\n",
    "predicted = []\n",
    "def set_predict(df):\n",
    "    predicted.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac18994dbb7434e8e8694d8580451a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2500000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbar = tqdm(total=len(valid))\n",
    "previous_test_df = None\n",
    "for (current_test, current_prediction_df) in iter_test:\n",
    "    if previous_test_df is not None:\n",
    "        previous_test_df[\"answered_correctly\"] = eval(\n",
    "            current_test[\"prior_group_answers_correct\"].iloc[0]\n",
    "        )\n",
    "        previous_test_df[previous_test_df.content_type_id == 0].groupby(\n",
    "            \"user_id\"\n",
    "        ).answered_correctly.apply(inference_dataset.update_answered_correctly)\n",
    "\n",
    "    # your feature extraction and model training code here\n",
    "    previous_test_df = current_test.copy()\n",
    "\n",
    "    # add current to cache\n",
    "    current_test = current_test[current_test.content_type_id == 0]\n",
    "    current_test = add_part_to_questions(current_test)\n",
    "    current_test[[\"row_id\", \"user_id\", \"content_id\", \"part\", \"timestamp\"]].groupby(\n",
    "        \"user_id\"\n",
    "    ).apply(lambda user_rows: inference_dataset.update_user_rows(user_rows))\n",
    "\n",
    "    # your prediction code here\n",
    "    current_test = predict_for_df(current_test)\n",
    "    set_predict(current_test.loc[:, [\"row_id\", \"answered_correctly\"]])\n",
    "    pbar.update(len(current_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation auc: 0.7301190955694308\n"
     ]
    }
   ],
   "source": [
    "#validation score\n",
    "y_pred = pd.concat(predicted).answered_correctly\n",
    "y_true = valid[valid.content_type_id == 0].answered_correctly[:len(y_pred)]\n",
    "\n",
    "print('validation auc:',roc_auc_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
